{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Predicting Student Dropout and Academic Success\"\n",
        "authors: \n",
        "    - \"Patricia Götz\"\n",
        "    - \"Lana Kabbani\"\n",
        "    - \"Noémie Glaus\"\n",
        "    - \"Estela Gonzalez Vizcarra\"\n",
        "institute: University of Lausanne\n",
        "date: today\n",
        "title-block-banner: \"#0095C8\"\n",
        "bibliography: reference.bib\n",
        "csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\n",
        "format:\n",
        "  html:\n",
        "    theme: cosmo\n",
        "    toc: true\n",
        "    toc-depth: 4\n",
        "    code-fold: true\n",
        "    code-tools: true\n",
        "    df-print: paged\n",
        "    self-contained: true\n",
        "  pdf:\n",
        "    toc: false\n",
        "    echo: false\n",
        "    include-in-header:\n",
        "      text: |\n",
        "        \\usepackage{fvextra}\n",
        "        \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{\n",
        "          commandchars=\\\\\\{\\},\n",
        "          breaklines, breaknonspaceingroup, breakanywhere\n",
        "        }\n",
        "    \n",
        "execute:\n",
        "  warning: false\n",
        "  message: false\n",
        "---\n",
        "\n",
        "## 1. - Introduction\n",
        "\n",
        "Student retention and academic success are crucial challenges for higher education institutions worldwide. Recent international observations show rising university dropout trends across multiple regions, including Australia and the United States [@sokolova2024dropout]. Looking closer at Europe, recent data from the German Center for Higher Education Research and Science Studies (2022), show that almost 30% of bachelor’s students in Germany leave university without graduating [@hachmeister2024german]. In Portugal, which is the focus of our analysis, recent data by Statistics Portugal reveal that a considerable portion of young adults (16.8%) aged from 15 to 34 have dropped out at least one level of education during their academic path [@europedata2024portugal]. Moreover, among those who dropped out, over more than half (50.8%) did not complete their tertiary studies, highlighting that higher education represents a critical point of disengagement [@europedata2024portugal].\n",
        "These figures underline the seriousness of dropouts in higher education and the reinforced need for universities to rely on data-driven insights to identify at-risk students and to design early intervention strategies. \n",
        "We chose this topic because predicting student dropout not only helps optimize institutional resources but also supports students in achieving their academic goals. Understanding the factors that influence academic success, such as socio-economic background, previous academic performance, or family situation, can improve educational policies and personalized support systems. This subject is particularly meaningful in data science, as it allows us to combine analytical and predictive methods to better understand and prevent student dropout.\n",
        "\n",
        "\n",
        "## 1.1 - Project Goals\n",
        "\n",
        "The main objective of this project is to identify the factors that influence students to drop out, stay enrolled, or graduate from higher education. The dataset provides detailed information on each student’s academic performance, socioeconomic background, and demographic profile, offering a comprehensive view of the variables that shape educational outcomes. By the end of our analysis, we seek to identify the most significant combinations of academic and personal factors that influence student success. \n",
        "First, our analysis will focus on academic performance, examining how variables such as admission grades, semester evaluations, and course results relate to final outcomes. For instance, we will analyze whether early academic performance can serve as a reliable predictor of future dropout risk. We will then explore the influence of socioeconomic and personal factors, including parental education, occupation, and financial situation, to understand their impact on academic achievement. Lastly,  the dataset will be used to build and evaluate classification models that predict students’ academic status (Dropout, Enrolled, or Graduate). \n",
        "In summary, this study combines exploratory analysis, visualization, and predictive modeling to generate actionable insights that help universities detect at-risk students early and strengthen academic success.\n",
        "\n",
        "---\n",
        "\n",
        "## 2 - Related Work\n",
        "\n",
        "As students' dropout is a major challenge in higher education, it represents a well-established area of research that has widely been studied in the literature over the years. Previous research articles have helped us acquire information about the topic, including the methods used in order to address the different research questions. \n",
        "\n",
        "One relevant study titled “Predicting Students’ Academic Success and Dropout Using Supervised Machine Learning” investigates the prediction of student academic success using supervised machine learning classification models. Throughout the paper, the authors compare multiple classification algorithms such as Decision Trees, Random Forest or Logistic Regression to assess their ability to predict student outcomes on student data. Their results show that these models are in fact an effective tool for identifying students at risk of dropping out and thus highlights the relevance of formulating this issue as a classification task. \n",
        "\n",
        "Other articles emphasize the importance of constructing robust predictive models, but also the role of feature selection. In particular a recent paper by Anaíle Mendes Rabelo and Luis Enrique Zarate (2024) demonstrates how combining academic performance indicators with contextual variables such as course selection, improves the reliability of dropout prediction models.\n",
        "\n",
        "In addition, an article published in 2022 named “Towards a Students’ Dropout Prediction Model in Higher Education Institutions Using Machine Learning Algorithms” focuses on the overall analytical pipeline used in educational data mining, from data preprocessing to model evaluation. The authors underline that data quality and preprocessing decisions play a key role in model performance. \n",
        "\n",
        "Overall, these research articles guided our methodological choices, particularly our choice of a classification framework, our focus on feature selection and our complete analysis process. Based on this literature, our project combines exploratory data analysis, predictive modeling, and interpretability techniques to better understand student outcomes.\n",
        "\n",
        "\n",
        "\n",
        "## 3 - Research Questions\n",
        "\n",
        "- I.  How do academic performance indicators and study conditions influence students’ likelihood of graduation or dropout?\n",
        "- II. What is the impact of demographic and socioeconomic background on students’ probability of dropping out?\n",
        "      a.  To what extent do financial factors (debtor status, scholarship holder) affect student retention ?\n",
        "- III. Can we accurately predict a student’s final status (Dropout, Enrolled, or Graduate) based on their demographic, socioeconomic, and academic characteristics. Which are the most relevant among them?\n",
        "        a.Which features category, academic (grades, units), socioeconomic (debt, scholarship) or demographic (age, gender)  contribute the most in predicting students’ dropout?\n",
        "\n",
        "## 4 - Data \n",
        "\n",
        "## 4.1 - Data Sourcing\n",
        "The dataset is publicly available on UCI Machine Learning Repository and was created from multiple databases of higher education institutions in Portugal. It is related to enrolled students in different undergraduate programs and shows how different demographic, socioeconomic and academic factors are related to the dropout. Since the data has already been collected and can be directly downloaded from [UCI MLR - Predict Students' Dropout and Academic Success](https://archive.ics.uci.edu/dataset/697/predict%2Bstudents%2Bdropout%2Band%2Bacademic%2Bsuccess){target=\"_blank\"} - [Accessed on 20th October] , there is no need to collect more data via webscraping or APIs. \n",
        "\n",
        "## 4.2 - Data Description\n",
        "The dataset, containing data from a Portuguese higher education institution, is provided as a CSV file, approximately 520 KB in size, and contains detailed information about students’demographic, academic and socio-economic characteristics. It includes 4424 student records and 37 variables (features). After reviewing the dataset variables, we removed two irrelevant ones, resulting in 35 relevant variables selected  for analysis.\n",
        "\n",
        "### 4.2.1 - Data Loading"
      ],
      "id": "e530d644"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup\n",
        "\n",
        "# Import libraries\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# Load data\n",
        "dataset = fetch_ucirepo(id=697)\n",
        "X = np.array(dataset.data.features)\n",
        "y = np.array(dataset.data.targets)\n",
        "\n",
        "# Create dataframe\n",
        "col_names = dataset.variables[\"name\"]\n",
        "df = pd.DataFrame(np.column_stack((X, y)), columns=col_names)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")"
      ],
      "id": "setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 4.2.2 - Variable Selection\n",
        "\n",
        "We selected 35 relevant variables for analysis:"
      ],
      "id": "1e3c795b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: variable-selection\n",
        "\n",
        "selected_columns = [ \n",
        "    \"Marital Status\", \n",
        "    \"Application order\", \n",
        "    \"Course\", \n",
        "    \"Daytime/evening attendance\", \n",
        "    \"Previous qualification\", \n",
        "    \"Previous qualification (grade)\",\n",
        "    \"Nacionality\", \n",
        "    \"Mother's qualification\", \n",
        "    \"Father's qualification\", \n",
        "    \"Mother's occupation\", \n",
        "    \"Father's occupation\", \n",
        "    \"Admission grade\", \n",
        "    \"Educational special needs\", \n",
        "    \"Gender\", \n",
        "    \"Scholarship holder\", \n",
        "    \"Age at enrollment\", \n",
        "    \"Displaced\", \n",
        "    \"Debtor\", \n",
        "    \"International\", \n",
        "    \"Curricular units 1st sem (credited)\", \n",
        "    \"Curricular units 1st sem (enrolled)\", \n",
        "    \"Curricular units 1st sem (evaluations)\",\n",
        "    \"Curricular units 1st sem (approved)\", \n",
        "    \"Curricular units 1st sem (grade)\", \n",
        "    \"Curricular units 1st sem (without evaluations)\", \n",
        "    \"Curricular units 2nd sem (credited)\", \n",
        "    \"Curricular units 2nd sem (enrolled)\", \n",
        "    \"Curricular units 2nd sem (evaluations)\", \n",
        "    \"Curricular units 2nd sem (approved)\", \n",
        "    \"Curricular units 2nd sem (grade)\", \n",
        "    \"Curricular units 2nd sem (without evaluations)\", \n",
        "    \"Unemployment rate\", \n",
        "    \"Inflation rate\", \n",
        "    \"GDP\", \n",
        "    \"Target\", \n",
        "]\n",
        "\n",
        "df = df[selected_columns].copy()\n",
        "print(f\"Selected {len(selected_columns)} variables\")"
      ],
      "id": "variable-selection",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2.3 - Selected Variable Descriptions"
      ],
      "id": "b089e88a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: variable-descriptions\n",
        "#| echo: false\n",
        "# Create variable information table\n",
        "variable_info = pd.DataFrame({\n",
        "    'Variable': [\n",
        "        'Marital Status',\n",
        "        'Application order',\n",
        "        'Course',\n",
        "        'Daytime/evening attendance',\n",
        "        'Previous qualification',\n",
        "        'Previous qualification (grade)',\n",
        "        'Nacionality',\n",
        "        \"Mother's qualification\",\n",
        "        \"Father's qualification\",\n",
        "        \"Mother's occupation\",\n",
        "        \"Father's occupation\",\n",
        "        'Admission grade',\n",
        "        'Educational special needs',\n",
        "        'Gender',\n",
        "        'Scholarship holder',\n",
        "        'Age at enrollment',\n",
        "        'Displaced',\n",
        "        'Debtor',\n",
        "        'International',\n",
        "        'Curricular units 1st sem (credited)',\n",
        "        'Curricular units 1st sem (enrolled)',\n",
        "        'Curricular units 1st sem (evaluations)',\n",
        "        'Curricular units 1st sem (approved)',\n",
        "        'Curricular units 1st sem (grade)',\n",
        "        'Curricular units 1st sem (without evaluations)',\n",
        "        'Curricular units 2nd sem (credited)',\n",
        "        'Curricular units 2nd sem (enrolled)',\n",
        "        'Curricular units 2nd sem (evaluations)',\n",
        "        'Curricular units 2nd sem (approved)',\n",
        "        'Curricular units 2nd sem (grade)',\n",
        "        'Curricular units 2nd sem (without evaluations)',\n",
        "        'Unemployment rate',\n",
        "        'Inflation rate',\n",
        "        'GDP',\n",
        "        'Target'\n",
        "    ],\n",
        "    'Description': [\n",
        "        'Student marital status',\n",
        "        'Application preference order',\n",
        "        'Course taken by student',\n",
        "        'Attendance type (daytime or evening)',\n",
        "        'Type of previous qualification',\n",
        "        'Grade of previous qualification',\n",
        "        'Student nationality',\n",
        "        'Educational qualification of mother',\n",
        "        'Educational qualification of father',\n",
        "        'Occupation of mother',\n",
        "        'Occupation of father',\n",
        "        'Admission grade to the program',\n",
        "        'Whether student has special educational needs',\n",
        "        'Student gender',\n",
        "        'Whether student is scholarship holder',\n",
        "        'Age of student at enrollment',\n",
        "        'Whether student is displaced from home',\n",
        "        'Whether student is a debtor',\n",
        "        'Whether student is international',\n",
        "        'Credited units in 1st semester',\n",
        "        'Enrolled units in 1st semester',\n",
        "        'Number of evaluations in 1st semester',\n",
        "        'Approved units in 1st semester',\n",
        "        'Average grade in 1st semester',\n",
        "        'Units without evaluations in 1st semester',\n",
        "        'Credited units in 2nd semester',\n",
        "        'Enrolled units in 2nd semester',\n",
        "        'Number of evaluations in 2nd semester',\n",
        "        'Approved units in 2nd semester',\n",
        "        'Average grade in 2nd semester',\n",
        "        'Units without evaluations in 2nd semester',\n",
        "        'Unemployment rate at time of enrollment',\n",
        "        'Inflation rate at time of enrollment',\n",
        "        'GDP at time of enrollment',\n",
        "        'Student status (Dropout, Enrolled, or Graduate)'\n",
        "    ],\n",
        "    'Type': [\n",
        "        'Categorical',\n",
        "        'Categorical',\n",
        "        'Categorical',\n",
        "        'Categorical',\n",
        "        'Categorical',\n",
        "        'Numerical (Continuous)',\n",
        "        'Categorical',\n",
        "        'Categorical',\n",
        "        'Categorical',\n",
        "        'Categorical',\n",
        "        'Categorical',\n",
        "        'Numerical (Continuous)',\n",
        "        'Binary',\n",
        "        'Binary',\n",
        "        'Binary',\n",
        "        'Numerical (Discrete)',\n",
        "        'Binary',\n",
        "        'Binary',\n",
        "        'Binary',\n",
        "        'Numerical (Discrete)',\n",
        "        'Numerical (Discrete)',\n",
        "        'Numerical (Discrete)',\n",
        "        'Numerical (Discrete)',\n",
        "        'Numerical (Continuous)',\n",
        "        'Numerical (Discrete)',\n",
        "        'Numerical (Discrete)',\n",
        "        'Numerical (Discrete)',\n",
        "        'Numerical (Discrete)',\n",
        "        'Numerical (Discrete)',\n",
        "        'Numerical (Continuous)',\n",
        "        'Numerical (Discrete)',\n",
        "        'Numerical (Continuous)',\n",
        "        'Numerical (Continuous)',\n",
        "        'Numerical (Continuous)',\n",
        "        'Categorical'\n",
        "    ]\n",
        "})\n",
        "# Display table\n",
        "from IPython.display import Markdown, display\n",
        "# Create markdown table\n",
        "table_md = variable_info.to_markdown(index=False)\n",
        "display(Markdown(table_md))"
      ],
      "id": "variable-descriptions",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Through this step, we didn't encounter any difficult challenges. The dataset was already clean and encoded, so we didn't need to perform variable merging, one-hot encoding or ordinal encoding. We only had to convert categorical variables into readable labels to facilitate our visualization analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2.4 - Preprocessing (Data Cleaning and Wrangling)\n",
        "\n",
        "One of the most important steps in our project is data cleaning and wrangling. After running the code to check for missing values and undefined numerical data, we found that the dataset contains no missing values, no mistakes and no data entry mistakes.\n",
        "\n",
        "The dataset was already encoded, and we removed “Application mode” and “Tuition fees up to date” variables because they are not relevant to our research questions. Therefore we dropped two columns from the dataset.\n",
        "Ensuring that the numeric columns are numeric, categorical variables such as “Gender”, “Debtor”, “Displaced” , “Daytime/Evening attendance” were translated to readable string labels for analysis.\n",
        "Although we had a well-structured and clean dataset, our main challenge was to determine the reliability of our dataset. We verified if there were any missing values, spotting mistakes, and determined irrelevant variables for our analysis. We pursue our cleaning work with the conversion of the categorical variables. Therefore, the reliable dataset was ready to be analyzed. \n"
      ],
      "id": "5bf79be0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: data-cleaning\n",
        "\n",
        "def clean_dataframe(df, col_missing_thresh=0.30, row_missing_thresh=0.50):\n",
        "    \"\"\"Clean dataset with missing value handling.\"\"\"\n",
        "  \n",
        "    # Count number of NaNs\n",
        "    df = df.copy()\n",
        "    missing = df.isna().sum()\n",
        "    missing_data = missing[missing > 0]\n",
        "\n",
        "    if len(missing_data) > 0:\n",
        "        print(f\"\\n⚠️  Missing values found in {len(missing_data)} columns ({missing_data.sum():,} total)\\n\")\n",
        "        display(missing_data.to_frame('Count'))\n",
        "    else:\n",
        "        print(\"\\n✓ No missing values found!\")\n",
        "\n",
        "    print(f\"\\nShape after cleaning: {df.shape}\")\n",
        "    print(f\"Missing values: {df.isna().sum().sum()}\")\n",
        "        \n",
        "    # Drop columns with excessive missing\n",
        "    col_frac = df.isna().mean()\n",
        "    drop_cols = col_frac[col_frac > col_missing_thresh].index.tolist()\n",
        "    if drop_cols:\n",
        "        df.drop(columns=drop_cols, inplace=True)\n",
        "    \n",
        "    # Drop rows with excessive missing\n",
        "    row_frac = df.isna().mean(axis=1)\n",
        "    drop_rows = row_frac[row_frac > row_missing_thresh].index\n",
        "    if len(drop_rows):\n",
        "        df = df.drop(index=drop_rows).reset_index(drop=True)\n",
        "    \n",
        "    # Coerce numeric types\n",
        "    df = df.apply(lambda s: pd.to_numeric(s, errors=\"ignore\"))\n",
        "    \n",
        "    # Impute missing values\n",
        "    for col in df.select_dtypes(include=[np.number]).columns:\n",
        "        if df[col].isna().any():\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "    \n",
        "    for col in df.select_dtypes(include=[\"category\",\"object\"]).columns:\n",
        "        if df[col].isna().any():\n",
        "            mode = df[col].mode(dropna=True)\n",
        "            if not mode.empty:\n",
        "                df[col] = df[col].fillna(mode.iloc[0])\n",
        "    \n",
        "    return df\n",
        "\n",
        "df = clean_dataframe(df)\n",
        "print(f\"Shape after cleaning: {df.shape}\")\n",
        "print(f\"Missing values: {df.isna().sum().sum()}\")"
      ],
      "id": "data-cleaning",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although we had a well-structured and clean dataset, our main challenge was to determine the reliability of our dataset. We verified if there were any missing values, spotting mistakes, and determined irrelevant variables for our analysis. We pursue our cleaning work with the conversion of the categorical variables. Therefore, the reliable dataset was ready to be analyzed.\n",
        "\n",
        "---\n",
        "\n",
        "## 5 - Exploratory Data Analysis (EDA)\n",
        "\n",
        "In this section, we explore the dataset to understand the main characteristics of the variables and how they relate to student outcomes (Dropout, Enrolled, Graduate). The goal of the EDA is to identify patterns, detect anomalies, and determine which features are most informative for predicting dropout.\n",
        "\n",
        "## 5.1 - Target Variable\n",
        "We begin by examining the distribution of the target variable.\n",
        "The three student outcomes (Dropout, Enrolled, and Graduate) are highly imbalanced, with Graduates representing the largest group, followed by Dropouts, and a smaller proportion of Enrolled students."
      ],
      "id": "1950cbfb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: target-distribution\n",
        "\n",
        "# Recode target\n",
        "target_col = \"Target\"\n",
        "df[target_col] = df[target_col].replace({\n",
        "    0: \"Dropout\", \n",
        "    1: \"Enrolled\", \n",
        "    2: \"Graduate\"\n",
        "})\n",
        "df[target_col] = pd.Categorical(\n",
        "    df[target_col], \n",
        "    categories=[\"Dropout\", \"Enrolled\", \"Graduate\"], \n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "target_counts = df[target_col].value_counts()\n",
        "colors = ['#F46968', '#BCDCED', '#31709D']\n",
        "bars = ax.bar(range(len(target_counts)), target_counts.values, \n",
        "              color=colors, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "ax.set_xticks(range(len(target_counts)))\n",
        "ax.set_xticklabels(target_counts.index)\n",
        "ax.set_ylabel('Number of Students', fontsize=11)\n",
        "ax.set_title('Student Outcomes Distribution', fontsize=13, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add labels\n",
        "for i, v in enumerate(target_counts.values):\n",
        "    ax.text(i, v + 30, f'{v}\\n({v/len(df)*100:.1f}%)', \n",
        "            ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "target-distribution",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5.2 - Correlation Analysis"
      ],
      "id": "f09d0963"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-height": 12,
        "fig-width": 14
      },
      "source": [
        "#| label: correlation-matrix\n",
        "\n",
        "# Calculate correlations\n",
        "corr = df.corr(numeric_only=True)\n",
        "\n",
        "# Create heatmap\n",
        "plt.figure(figsize=(16, 14))\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
        "\n",
        "sns.heatmap(\n",
        "    corr, \n",
        "    mask=mask,\n",
        "    cmap='RdBu_r', \n",
        "    center=0,\n",
        "    vmin=-1, \n",
        "    vmax=1,\n",
        "    annot=True, \n",
        "    fmt='.2f',\n",
        "    square=True,\n",
        "    linewidths=0.5,\n",
        "    cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation\"}\n",
        ")\n",
        "\n",
        "plt.title('Correlation Matrix of Numeric Variables', \n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "correlation-matrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on our correlation analysis, we identified several moderately and highly correlated variable pairs that indicate multicollinearity.\n",
        "A high correlation between international and nationality students can be observed, therefore we choose to remove the variable _international_, since it won’t be as relevant as the _nacionality_ variable.\n",
        "We can see that the variables _father’s occupation_ and _mother’s occupation_  are highly correlated, but in this case the correlation reflects social structure. They represent two distinct individuals and two potentially different socioeconomic effects. Same thing applies for _mother’s qualification_ and _father’s qualification_.\n",
        "Although the variables _Curricular units 1st sem (enrolled)_ _Curricular units 2nd sem (enrolled)_ and _Curricular units 1st sem (grade)_ _Curricular units 2nd sem (grade)_ are respectively highly correlated, we keep them because they provide performance progression across different time periods, which is relevant for predicting dropout. Therefore, we excluded 8 redundant semester variables and one nationality variable.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 5.3 - Feature Selection"
      ],
      "id": "4cdb06f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: feature-selection\n",
        "\n",
        "# Remove highly correlated features\n",
        "columns_to_remove = [\n",
        "    \"Curricular units 1st sem (credited)\", \n",
        "    \"Curricular units 1st sem (evaluations)\", \n",
        "    \"Curricular units 1st sem (approved)\",\n",
        "    \"Curricular units 1st sem (without evaluations)\",\n",
        "    \"Curricular units 2nd sem (credited)\", \n",
        "    \"Curricular units 2nd sem (evaluations)\", \n",
        "    \"Curricular units 2nd sem (approved)\",\n",
        "    \"Curricular units 2nd sem (without evaluations)\",\n",
        "    \"International\",\n",
        "]\n",
        "\n",
        "df = df.drop(columns=columns_to_remove)\n",
        "print(f\"Removed {len(columns_to_remove)} highly correlated variables\")\n",
        "print(f\"Remaining variables: {df.shape[1]}\")"
      ],
      "id": "feature-selection",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5.4 - Outlier Detection\n",
        "\n",
        "We implemented a type-aware outlier detection strategy that applies different methods based on the nature of each variable:\n",
        "\n",
        "**Binary variables** (e.g., Gender, Scholarship holder): Outlier detection was skipped entirely, as these variables only contain two valid values (0/1).\n",
        "\n",
        "**Nominal categorical variables** (e.g., Course, Nationality): No outlier detection applied, as these represent distinct categories without natural ordering. We only reported the number of unique categories present.\n",
        "\n",
        "**Ordinal categorical variables** (e.g., qualifications, occupations): We reported the number of levels but did not apply outlier detection, as these represent ordered categories rather than continuous measurements.\n",
        "\n",
        "**Grade variables** (0-200 scale): We checked for values outside the valid range (0-200). According to the dataset documentation, grades in the Portuguese system can range from 0 to 200.\n",
        "\n",
        "**Count variables** (e.g., enrolled courses): We used a more lenient threshold of 3×IQR (Interquartile Range) rather than the standard 1.5×IQR, as count variables naturally exhibit right-skewed distributions where high values may represent legitimate cases (e.g., students enrolling in many courses).\n",
        "\n",
        "**Continuous variables** (e.g., Age, GDP, Unemployment rate): We applied the standard Tukey method with 1.5×IQR threshold to identify potential outliers: values below Q1 - 1.5×IQR or above Q3 + 1.5×IQR.\n",
        "\n",
        "This approach ensures that outlier detection is contextually appropriate for each variable type, reducing false positives while identifying genuine data quality issues."
      ],
      "id": "85cb193c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: outlier-detection\n",
        "\n",
        "def detect_outliers_intelligent(df, var_type_dict):\n",
        "    \"\"\"Detect outliers based on variable type using simple statistical rules.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Binary variables - skip\n",
        "    print(\"\\n Binary variables (skipping outlier detection):\")\n",
        "    for col in var_type_dict.get('binary', []):\n",
        "        if col in df.columns:\n",
        "            unique_vals = sorted(df[col].dropna().unique())\n",
        "            print(f\"  - {col}: values = {unique_vals}\")\n",
        "    \n",
        "    # Nominal categorical\n",
        "    print(\"\\n Nominal Categorical (no natural order):\")\n",
        "    for col in var_type_dict.get('nominal', []):\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "        series = df[col].dropna()\n",
        "        print(f\"  - {col}: {len(series.unique())} categories\")\n",
        "    \n",
        "    # Ordinal categorical\n",
        "    print(\"\\n Ordinal Categorical (meaningful order):\")\n",
        "    for col in var_type_dict.get('ordinal', []):\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "        series = df[col].dropna()\n",
        "        print(f\"  - {col}: {len(series.unique())} levels\")\n",
        "    \n",
        "    # Grade variables (0-200 scale + Z-score)\n",
        "    print(\"\\n Grade variables (0-200 range + Z-score > 3):\")\n",
        "    for col in var_type_dict.get('grades', []):\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "        series = df[col].dropna()\n",
        "        \n",
        "        # Check range violations\n",
        "        invalid = ((series < 0) | (series > 200)).sum()\n",
        "        \n",
        "        # Check statistical outliers using Z-score\n",
        "        mean, std = series.mean(), series.std()\n",
        "        if std > 0:\n",
        "            z_scores = np.abs((series - mean) / std)\n",
        "            statistical_outliers = (z_scores > 3).sum()\n",
        "        else:\n",
        "            statistical_outliers = 0\n",
        "        \n",
        "        total_outliers = invalid + statistical_outliers\n",
        "        outlier_pct = 100 * total_outliers / len(series) if len(series) > 0 else 0\n",
        "        \n",
        "        print(f\"  - {col}: {invalid} out-of-range + {statistical_outliers} extreme (Z>3) = \"\n",
        "              f\"{total_outliers} total ({outlier_pct:.1f}%)\")\n",
        "        \n",
        "        if total_outliers > 0:\n",
        "            results.append({\n",
        "                'column': col, 'type': 'grade', \n",
        "                'issue': 'out_of_range + extreme',\n",
        "                'count': total_outliers, 'pct': outlier_pct\n",
        "            })\n",
        "    \n",
        "    # Count variables (Z-score > 3)\n",
        "    print(\"\\n Count variables (Z-score > 3):\")\n",
        "    for col in var_type_dict.get('counts', []):\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "        series = df[col].dropna()\n",
        "        if len(series) == 0:\n",
        "            continue\n",
        "        \n",
        "        mean, std = series.mean(), series.std()\n",
        "        if std > 0:\n",
        "            z_scores = np.abs((series - mean) / std)\n",
        "            outliers = (z_scores > 3).sum()\n",
        "        else:\n",
        "            outliers = 0\n",
        "        \n",
        "        outlier_pct = 100 * outliers / len(series)\n",
        "        \n",
        "        print(f\"  - {col}: extreme values: {outliers} ({outlier_pct:.1f}%)\")\n",
        "        \n",
        "        if outliers > 0:\n",
        "            results.append({\n",
        "                'column': col, 'type': 'count', \n",
        "                'issue': 'extreme_outlier',\n",
        "                'count': outliers, 'pct': outlier_pct\n",
        "            })\n",
        "    \n",
        "    # Continuous variables (Z-score > 3)\n",
        "    print(\"\\n Continuous variables (Z-score > 3):\")\n",
        "    for col in var_type_dict.get('continuous', []):\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "        series = df[col].dropna()\n",
        "        if len(series) == 0:\n",
        "            continue\n",
        "        \n",
        "        mean, std = series.mean(), series.std()\n",
        "        if std > 0:\n",
        "            z_scores = np.abs((series - mean) / std)\n",
        "            outliers = (z_scores > 3).sum()\n",
        "        else:\n",
        "            outliers = 0\n",
        "        \n",
        "        outlier_pct = 100 * outliers / len(series)\n",
        "        \n",
        "        print(f\"  - {col}: extreme values: {outliers} ({outlier_pct:.1f}%)\")\n",
        "        \n",
        "        if outliers > 0:\n",
        "            results.append({\n",
        "                'column': col, 'type': 'continuous', \n",
        "                'issue': 'extreme_outlier',\n",
        "                'count': outliers, 'pct': outlier_pct\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Define variable types\n",
        "var_types = {\n",
        "    'binary': [\n",
        "        \"Daytime/evening attendance\", \"Educational special needs\", \n",
        "        \"Gender\", \"Scholarship holder\", \"Displaced\", \"Debtor\", \"International\"\n",
        "    ],\n",
        "    'nominal': [\"Course\", \"Nacionality\"],\n",
        "    'ordinal': [\n",
        "        \"Marital Status\", \"Application mode\", \"Application order\",\n",
        "        \"Previous qualification\", \"Mother's qualification\", \n",
        "        \"Father's qualification\", \"Mother's occupation\", \"Father's occupation\"\n",
        "    ],\n",
        "    'grades': [\n",
        "        \"Previous qualification (grade)\", \"Admission grade\",\n",
        "        \"Curricular units 1st sem (grade)\", \"Curricular units 2nd sem (grade)\"\n",
        "    ],\n",
        "    'counts': [\n",
        "        \"Curricular units 1st sem (enrolled)\",\n",
        "        \"Curricular units 2nd sem (enrolled)\"\n",
        "    ],\n",
        "    'continuous': [\"Age at enrollment\", \"Unemployment rate\", \"Inflation rate\", \"GDP\"]\n",
        "}\n",
        "\n",
        "# Run outlier detection\n",
        "outlier_results = detect_outliers_intelligent(df, var_types)"
      ],
      "id": "outlier-detection",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4.1 - Outlier Summary"
      ],
      "id": "58e1f5d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: outlier-summary\n",
        "\n",
        "if not outlier_results.empty:\n",
        "    outlier_results = outlier_results.sort_values('pct', ascending=False)\n",
        "    print(\"\\n Detected Issues:\")\n",
        "    outlier_results\n",
        "    \n",
        "    # Visualize problematic variables\n",
        "    for _, row in outlier_results.iterrows():\n",
        "        col = row['column']\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "        \n",
        "        # Histogram\n",
        "        df[col].hist(bins=30, ax=ax1, edgecolor='black')\n",
        "        ax1.set_title(f\"Distribution\")\n",
        "        ax1.set_xlabel(col)\n",
        "        ax1.set_ylabel(\"Frequency\")\n",
        "        ax1.grid(alpha=0.3)\n",
        "        \n",
        "        # Boxplot\n",
        "        sns.boxplot(y=df[col], ax=ax2)\n",
        "        ax2.set_title(f\"Boxplot ({row['type']})\")\n",
        "        ax2.grid(alpha=0.3, axis='y')\n",
        "        \n",
        "        plt.suptitle(f\"{col}: {row['count']} potential outliers ({row['pct']:.1f}%)\", \n",
        "                     fontsize=12, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"\\n No significant outliers detected!\")"
      ],
      "id": "outlier-summary",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We identified outliers in five different variables. _Curricular units 1st semester (enrolled)_ and _Curricular units 2nd semester (enrolled)_, which represent the number of courses students register for each semester. We observed 106 potential outliers in the first curricular semester and 82 in the second semester. Since the average course load is usually 5 to 6 classes, students taking a much higher or lower number of courses are naturally flagged as outliers. In the first semester, the highest value reaches 26 classes. Although this is an ambitious workload, it remains possible. Several situations could explain such a high number: for instance, a student trying to complete their degree quickly, or a student retaking courses after previous failures. These cases can reflect meaningful academic behaviours, so removing them would risk losing useful information. For the second semester, the maximum value is around 20 classes, leading to similar conclusions. In both semesters, we also observe students enrolled in zero courses, which appears as an extreme value as well. This may correspond to students who completed most of their required courses earlier, or students taking a temporary break while still being officially enrolled. These profiles are still relevant and should be included. In this context, these extreme values are not problematic. On the contrary, they may help us understand whether taking unusually many, or unusually few, courses has an impact on college dropout. For this reason, we decided not to remove or cap these observations.\n",
        "The next variable with detected outliers is _Age at enrollment_ , for which 101 potential outliers were identified. Since the average age at enrollment is around 20 years old, students beginning their studies at 40 or 50 naturally appear as unusual cases.The oldest student is 70 years old, which, while rare, is no need for concern regarding methodology within our analysis. Being 70 years old is no different regarding being classified as a student and this data point should be included. These values represent real and meaningful student profiles, such as mature students or individuals returning to education after a long break. Excluding them would remove important diversity from the dataset and limit our understanding of the different types of students who may or may not drop out. For this reason, we chose not to remove or limit the age-related outliers. Finally, outliers were also detected in Admission grade (22 cases) and Previous qualification grade (21 cases). These extreme values reflect either exceptionally high academic performance or, conversely, unusually low grades. Since these cases may provide insights into how prior academic achievement relates to dropout behavior, removing or capping them would not be appropriate. We therefore opted to retain all outliers in these grade variables as well. Based on our research questions, we conclude that removing these outliers would not benefit our analysis, as they do not represent errors but rather uncommon yet meaningful observations. Retaining them allows us to capture the full diversity of student profiles and provides a more accurate understanding of the factors that may influence college dropout.\n",
        "\n",
        "\n",
        "## 6 - Feature Importance Analysis\n",
        "\n",
        "### 6.1 - Methodology\n",
        "\n",
        "We used one-way ANOVA (Analysis of Variance) to identify which numeric variables show significant differences across the three target groups (Dropout, Enrolled, Graduate). For each variable, we calculated:\n",
        "\n",
        "- **p-value**: Statistical significance of differences between groups (α = 0.05)\n",
        "- **Eta-squared (η²)**: Effect size measure representing the proportion of variance explained by the target variable (ranges from 0 to 1, where higher values indicate stronger association)\n",
        "\n",
        "Variables with p-value < 0.05 are considered significantly associated with student outcomes and may be strong predictors in classification models."
      ],
      "id": "6543307f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: anova-analysis\n",
        "\n",
        "# ANOVA for numeric variables\n",
        "anova_results = {}\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "\n",
        "for col in numeric_cols:\n",
        "    groups = [df.loc[df[target_col] == cat, col].dropna()\n",
        "              for cat in df[target_col].cat.categories\n",
        "              if cat in df[target_col].unique()]\n",
        "    \n",
        "    # Need at least 2 non-empty groups\n",
        "    if sum(len(g) > 0 for g in groups) < 2:\n",
        "        continue\n",
        "    \n",
        "    from scipy.stats import f_oneway\n",
        "    f_val, p_val = f_oneway(*groups)\n",
        "    \n",
        "    # Effect size: eta-squared\n",
        "    grand_mean = df[col].mean()\n",
        "    ss_between = sum(len(g) * (g.mean() - grand_mean) ** 2 for g in groups)\n",
        "    ss_total = ((df[col] - grand_mean) ** 2).sum()\n",
        "    eta_sq = ss_between / ss_total if ss_total > 0 else np.nan\n",
        "    \n",
        "    anova_results[col] = {\"p_value\": p_val, \"eta_sq\": eta_sq}\n",
        "\n",
        "# Create results dataframe\n",
        "anova_df = (pd.DataFrame(anova_results).T\n",
        "            .sort_values([\"p_value\", \"eta_sq\"], ascending=[True, False]))\n",
        "anova_df[\"significant\"] = anova_df[\"p_value\"] < 0.05\n",
        "\n",
        "print(f\"Significant variables (p < 0.05): {anova_df['significant'].sum()}\")\n",
        "anova_df.head(15)"
      ],
      "id": "anova-analysis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When evaluating feature importance, both statistical significance (p-value) and practical significance (effect size) must be considered. With large sample sizes, even trivial differences can reach statistical significance, making effect size interpretation essential.\n",
        "\n",
        "**Effect Size (η²)** measures the proportion of variance in student outcomes explained by each feature, with interpretations:\n",
        "\n",
        "- **η² = 0.01** (1%): Small effect\n",
        "- **η² = 0.06** (6%): Medium effect  \n",
        "- **η² = 0.14** (14%): Large effect\n",
        "\n",
        "For example, marital status has a highly significant p-value (2.66e-09) but explains less than 1% of variance (η² = 0.009), indicating negligible practical importance. In contrast, \"Curricular units 2nd sem (grade)\" explains 34% of variance (η² = 0.339), representing a large and meaningful effect. When identifying important predictors, it's better to prioritize features with larger effect sizes rather than relying only on p-values. \n",
        "\n",
        "## 6.2 - Top Predictive Variables\n",
        "\n",
        "### 6.2.1 - Acadamic Performance Indicators\n",
        "\n",
        "Our exploratory analysis shows relationships between academic performance measures and student outcomes (Dropout, Enrolled, Graduate). Several patterns emerge across admission grades, semester performance, and course load."
      ],
      "id": "113e50ea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-admission-grade\n",
        "#| fig-cap: Admission grade by student outcome\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(\n",
        "    x=target_col,\n",
        "    y='Admission grade',\n",
        "    data=df,\n",
        "    palette=['#F46968', '#BCDCED', '#326E9E']\n",
        ")\n",
        "plt.title('Admission grade by Target', fontsize=12, fontweight='bold')\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-admission-grade",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In Figure 1, we observe the distribution of the admission grade across the three categories (Dropout, Enrolled Target, and Graduate). Dropout students have an average admission grade of around 122, with several outliers reaching above 160. Enrolled Target students show a very similar average grade to Dropout students, but with fewer extreme values. Graduate students display a slightly higher average admission grade, around 125, and similarly present a few outliers above 160.\n",
        "Overall, the three groups show comparable distributions, with considerable overlap in their admission grades. Graduate students tend to have a marginally higher average, which may suggest that stronger academic preparation is associated with a greater likelihood of graduating. However, the presence of high admission grades in both the Dropout and Graduate categories indicates that good grades alone do not fully determine academic outcomes. In other words, while admission grade may play a role, it is not a decisive predictor of whether a student will graduate or drop out.\n"
      ],
      "id": "520335d9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-prev-qual-grade\n",
        "#| fig-cap: Previous qualification grade by student outcome\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(\n",
        "    x=target_col,\n",
        "    y='Previous qualification (grade)',\n",
        "    data=df,\n",
        "    palette=['#F46968', '#BCDCED', '#326E9E']\n",
        ")\n",
        "plt.title('Previous qualification (grade) by Target', fontsize=12, fontweight='bold')\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-prev-qual-grade",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure 2 shows the distribution of the Previous qualification (grade) across the three target students which are Dropout, Enrolled, and Graduate. All three boxplots display similar characteristics, with medians around 130-133. The minimum and maximum values are also comparable from 100 to 165. The three groups have multiple outliers at both lower and upper extremes of the grade distribution, dropout and graduates are the one that show more extreme values.\n",
        "\n",
        "\n",
        "For the interpretation, as the distributions and medians are quite similar this suggests that previous qualification grade is not a strong predictor of students' performance. Interestingly the Dropout group’s median is quite high which indicates that students who drop out have not necessarily lower prior grades than those who graduate or stay enrolled.The outliers indicate that in each category there are both very high and very low grades, which suggests that there are other factors beyond academic performance.\n"
      ],
      "id": "ceb8d92f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-st-sem-grade\n",
        "#| fig-cap: First semester grade by student outcome\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(\n",
        "    x=target_col,\n",
        "    y='Curricular units 1st sem (grade)',\n",
        "    data=df,\n",
        "    palette=['#F46968', '#BCDCED', '#326E9E']\n",
        ")\n",
        "plt.title('Curricular units 1st sem (grade) by Target', fontsize=12, fontweight='bold')\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-st-sem-grade",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In Figure 3, we see the first-semester grades for the three target groups: Dropout, Enrolled Target, and Graduate. Dropout students show a wide range of grades. Enrolled Target students have grades around a median of 12.5, with moderate spread. Graduate students have the highest median, around 13.5, and a tighter distribution.\n",
        "For interpretation, the wide spread of Dropout students suggests that leaving the program is not only due to low grades. Enrolled Target students show average performance, indicating steady progress but not full completion. Graduate students perform consistently better, suggesting that higher and more stable first-semester grades are associated with graduation.\n"
      ],
      "id": "f8b295d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-boxplot-grades\n",
        "#| fig-cap: Boxplot of Grades by Target\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "sns.boxplot(\n",
        "    x=target_col,\n",
        "    y='Curricular units 2nd sem (grade)',\n",
        "    data=df,\n",
        "    palette=['#F46968', '#BCDCED', '#326E9E']\n",
        ")\n",
        "\n",
        "plt.title(f\"Curricular units 2nd sem (grade) by {target_col}\", fontsize=12, fontweight='bold')\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-boxplot-grades",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure 4 reveals distinct patterns across the three groups. The Dropout category displays the widest range of performance. Enrolled students demonstrate moderate variability with a median near 12 units. Graduates show the tightest distribution and highest median at approximately 13 units. By the second semester, the gaps between groups widen. Many dropouts completed few or no units (the distribution starts at 0), indicating this is likely when they left the program. Graduates continued performing well with consistent results around 13 units. Enrolled students fell somewhere in between with decent but mixed performance. The second semester appears to be a turning point where struggling students drop out while successful students keep their momentum."
      ],
      "id": "06c03fa5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-1st-sem-enrolled\n",
        "#| fig-cap: First semester enrollment by student outcome\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(\n",
        "    x=target_col,\n",
        "    y='Curricular units 1st sem (enrolled)',\n",
        "    data=df,\n",
        "    palette=['#F46968', '#BCDCED', '#326E9E']\n",
        ")\n",
        "plt.title('Curricular units 1st sem (enrolled) by Target', fontsize=12, fontweight='bold')\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-1st-sem-enrolled",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure 5 demonstrates the relationship between Curricular units 1st Sem (enrolled) and the three Target outcomes (Dropout, Enrolled, Graduate). All three groups show similar box positions with medians around 5-6 units. Dropouts and Enrolled students have nearly identical distributions, while Graduates have a slightly higher box position. All groups show numerous outliers, particularly on the upper end, with some students enrolling in 15-26 units.\n",
        "Figure 5 reveals that the number of courses taken is not a factor influencing different outcomes, since all groups show similar enrollment patterns. Many high outliers appear across all groups, suggesting that ambitious enrollment is common regardless of eventual outcome.\n"
      ],
      "id": "e4b5e9e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-2nd-sem-enrolled\n",
        "#| fig-cap: Second semester enrollment by student outcome\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(\n",
        "    x=target_col,\n",
        "    y='Curricular units 2nd sem (enrolled)',\n",
        "    data=df,\n",
        "    palette=['#F46968', '#BCDCED', '#326E9E']\n",
        ")\n",
        "plt.title('Curricular units 2nd sem (enrolled) by Target', fontsize=12, fontweight='bold')\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-2nd-sem-enrolled",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As shown in Figure 6, Dropout and Enrolled students have similar distributions with their boxes positioned in the lower range. Graduate students show a noticeably higher box position and a wider spread. All three groups display numerous outliers, particularly on the upper end.\n",
        "Like the 1st semester enrollment patterns, the 2nd semester shows that graduates tend to enroll in slightly more courses, though the differences remain modest. The similar enrollment behavior between dropouts and enrolled students suggests that course load decisions in the 2nd semester don't strongly differentiate these groups - the key difference lies in completion rates rather than enrollment ambitions.\n"
      ],
      "id": "36a89448"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-daytime-evening\n",
        "#| fig-cap: Daytime/evening attendance by student outcome\n",
        "\n",
        "tab = (pd.crosstab(df[target_col], df['Daytime/evening attendance'])\n",
        "       .apply(lambda r: r / r.sum(), axis=1))\n",
        "tab = tab.reindex(columns=sorted(tab.columns.tolist()))\n",
        "\n",
        "tab.plot(kind=\"bar\", stacked=True,\n",
        "        color=['#F46968', '#8BC68A'], edgecolor='black')\n",
        "plt.ylabel(\"Proportion within target group\")\n",
        "plt.title('Daytime/evening attendance by Target', fontsize=12, fontweight='bold')\n",
        "plt.legend(title='Attendance', labels=['Evening', 'Daytime'],\n",
        "         bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-daytime-evening",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure 7 shows the proportion of daytime and evening attendance within the three groups (Dropout, Enrolled, Graduate). Daytime attendance dominates across all three groups, representing approximately 85-90% of students. However, Dropout students show a slightly higher proportion of evening attendance (around 15%) compared to Enrolled and Graduate students (around 10%). \n",
        "This small difference might indicate that evening students face additional challenges, though the similarity across all groups suggests attendance timing is not a primary driver of dropout rates.\n"
      ],
      "id": "f487b683"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-application-order\n",
        "#| fig-cap: Application order by student outcome\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(\n",
        "    x=target_col,\n",
        "    y='Application order',\n",
        "    data=df,\n",
        "    palette=['#F46968', '#BCDCED', '#326E9E']\n",
        ")\n",
        "plt.title('Application order by Target', fontsize=12, fontweight='bold')\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-application-order",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure 8 shows the Application order by the three target students. All three groups show similar distributions and are positioned in the lower range. The medians are approximately 1.5-2 for all categories. The upper whisker is similar for the three groups, reaching 3 and the lower whisker is at 0 for Graduates and around 1 for Dropout and Enrolled. There are numerous outliers that are at 4, 5 and 6, and even 9 for Enrolled category, indicating that some students applied as their 4th, 5th, 6th and 9th choice.\n",
        "Regarding the interpretation, as the distribution is similar in the three categories this implies that the application order has not a strong relationship with students' success. Most students have applied to this institution as their first or second choice, suggesting that institutional preferences do not really predict if a student will drop out, stay enrolled or graduate. We can also confirm that, as the outliers are similar, the application order is not a meaningful predictor of a student's performance.\n"
      ],
      "id": "a3259964"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-displaced\n",
        "#| fig-cap: Displaced status by student outcome\n",
        "\n",
        "tab = (pd.crosstab(df[target_col], df['Displaced'])\n",
        "       .apply(lambda r: r / r.sum(), axis=1))\n",
        "tab = tab.reindex(columns=sorted(tab.columns.tolist()))\n",
        "\n",
        "tab.plot(kind=\"bar\", stacked=True,\n",
        "        color=['#F46968', '#8BC68A'], edgecolor='black')\n",
        "plt.ylabel(\"Proportion within target group\")\n",
        "plt.title('Displaced by Target', fontsize=12, fontweight='bold')\n",
        "plt.legend(title='Displaced', labels=['No', 'Yes'],\n",
        "         bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-displaced",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure 9 shows the proportion of displaced students (those who moved or changed residence) across the three target groups. Dropout students have the highest proportion of non-displaced students at around 53%. Enrolled students show about 45% non-displaced. Graduate students have the lowest at approximately 40% non-displaced, meaning 60% of graduates relocated.\n",
        "The pattern shows that students who relocated for their studies were more likely to graduate. This could be because moving demonstrates stronger commitment to education, or because staying home means dealing with work, family responsibilities, or other obligations that interfere with studying. Dropouts were the least likely to have relocated, suggesting that remaining in their original environment may have made it harder to focus on academics.\n",
        "\n",
        "\n",
        "### 6.2.2 - Key Findings for Academic Performance and Study Conditions\n",
        "\n",
        "Graduates have higher admission grades and previous qualification grades compared to dropouts, though the differences are relatively small. This demonstrates that prior academic preparation shows limited predictive power.\n",
        "\n",
        "\n",
        "First-semester grades are the strongest predictor of students' performance. Students who drop out show dramatically lower grades (many between 0-5), while graduates consistently have higher grades (median around 12). First semester performance is therefore a critical warning signal for identifying at-risk students.\n",
        "\n",
        "\n",
        "Graduates tend to enroll in more courses in the first semester (median around 6-7) compared to those who drop out (median around 5-6), this may reflect a stronger initial academic engagement, even though this difference remains small.\n",
        "\n",
        "\n",
        "Daytime/evening attendance suggests an observable difference and proves to be an important predictor. Evening students show higher drop out rates, around 15% of dropouts compared to 10% for graduates. This reflects additional challenges faced by students who must balance work, or family responsibilities with their studies.\n",
        "\n",
        "\n",
        "Students who are displaced have higher graduation rates (60% of graduates vs around 48% of dropouts). This counter-intuitive pattern reflects that relocating for studies may reflect stronger commitment or independence.\n",
        "\n",
        "\n",
        "### 6.2.3 - Demographic & Socioeconomic Background\n"
      ],
      "id": "f0cecda9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-age\n",
        "#| fig-cap: Age at enrollment by student outcome\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(\n",
        "    x=target_col,\n",
        "    y='Age at enrollment',\n",
        "    data=df,\n",
        "    palette=['#F46968', '#BCDCED', '#31709D']\n",
        ")\n",
        "plt.title('Age at enrollment by Target', fontsize=12, fontweight='bold')\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-age",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure 10 demonstrates the relationship between Age at enrollment and the three students outcomes. The Dropout group has the highest median age, which is approximately 23 and the widest interquartile range. The Enrolled group has a median age around 20-21, while the Graduate group shows the lowest median age at around 19. It is also the narrowest. The three groups contain numerous outliers, showing particularly older students from late 30th to 70 years old.\n",
        "\n",
        "\n",
        "This suggests that age at enrollment is a significant predictor of students' performance. We see that students who enroll at a younger age are more likely to graduate, while older students face more risk of dropping out. This can be caused by several factors, such as the fact that younger students may have fewer external responsibilities compared to older students that may deal with multiple commitments that can interfere with their studies. The wider distribution dropout’s group shows that students can occur at any age. However, older students do successfully graduate, showing that age does not determine success alone."
      ],
      "id": "bc9f1349"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-gender\n",
        "#| fig-cap: Gender by student outcome\n",
        "\n",
        "tab = (pd.crosstab(df[target_col], df['Gender'])\n",
        "       .apply(lambda r: r / r.sum(), axis=1))\n",
        "tab = tab.reindex(columns=sorted(tab.columns.tolist()))\n",
        "\n",
        "tab.plot(kind=\"bar\", stacked=True,\n",
        "        color=['#F8A88E', '#8CB3D5'], edgecolor='black')\n",
        "plt.ylabel(\"Proportion within target group\")\n",
        "plt.title('Gender by Target', fontsize=12, fontweight='bold')\n",
        "plt.legend(title='Gender', labels=['Female', 'Male'],\n",
        "         bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-gender",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure 11 shows the proportion of genders within each of the three target groups (Dropout, Enrolled Target, Graduate). In the Dropout group, the proportion of male and female students is almost equal. In contrast, both the Enrolled Target and Graduate groups have a higher proportion of female students than male students. \n",
        "\n",
        "\n",
        "This suggests that female students tend to persist and complete their studies at higher rates than male students. Male students appear slightly more likely to interrupt or drop out of their programs, which may contribute to the lower proportions observed in the Enrolled Target and Graduate groups."
      ],
      "id": "799f32c1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-scholarship\n",
        "#| fig-cap: Scholarship holder status by student outcome\n",
        "\n",
        "#plt.figure(figsize=(8, 5))\n",
        "tab = (pd.crosstab(df[target_col], df['Scholarship holder'])\n",
        "       .apply(lambda r: r / r.sum(), axis=1))\n",
        "tab = tab.reindex(columns=sorted(tab.columns.tolist()))\n",
        "\n",
        "tab.plot(kind=\"bar\", stacked=True, \n",
        "        color=['#F46968', '#8BC68A'], edgecolor='black')\n",
        "plt.ylabel(\"Proportion within target group\")\n",
        "plt.title('Scholarship holder by Target', fontsize=12, fontweight='bold')\n",
        "plt.legend(title='Scholarship holder', labels=['No', 'Yes'], \n",
        "         bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-scholarship",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure 12 shows the proportion of scholarship holders within each target group (Dropout, Enrolled Target, Graduate).\n",
        "In both the Dropout and Enrolled Target groups, the vast majority of students do not receive a scholarship, with only a small proportion being scholarship holders. In contrast, the Graduate group contains a noticeably higher proportion of scholarship recipients.\n",
        "This suggests that students who receive a scholarship may be more likely to graduate than those who do not. Scholarships often reduce financial pressure and provide support that may help students remain enrolled and complete their studies. Conversely, students without scholarships seem more represented among dropouts and ongoing enrollments.\n"
      ],
      "id": "3f6a69d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-debtor\n",
        "#| fig-cap: Debtor status by student outcome\n",
        "\n",
        "tab = (pd.crosstab(df[target_col], df['Debtor'])\n",
        "       .apply(lambda r: r / r.sum(), axis=1))\n",
        "tab = tab.reindex(columns=sorted(tab.columns.tolist()))\n",
        "\n",
        "tab.plot(kind=\"bar\", stacked=True,\n",
        "        color=['#F46968', '#8BC68A'], edgecolor='black')\n",
        "plt.ylabel(\"Proportion within target group\")\n",
        "plt.title('Debtor by Target', fontsize=12, fontweight='bold')\n",
        "plt.legend(title='Debtor', labels=['No', 'Yes'],\n",
        "         bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-debtor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Figure 13 shows that the dropout group has the largest proportion of students who are debtors. Enrolled students still include some debtors, but the proportion is noticeably smaller. In the graduate group, almost all students have no debt, with only a very small fraction appearing as debtors.\n",
        "\n",
        "\n",
        "This trend suggests that having debt is more common among students who end up dropping out, hinting that financial pressure may contribute to early departure. Conversely, students without debt seem more likely to remain enrolled and reach graduation.\n"
      ],
      "id": "39d0b9b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-marital-status\n",
        "#| fig-cap: Marital status by student outcome\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Encode target categories as numbers\n",
        "x_encoded = df[target_col].cat.codes\n",
        "\n",
        "# Add jitter to avoid overlap\n",
        "x_jitter = x_encoded + np.random.normal(0, 0.05, size=len(df))\n",
        "y_jitter = df['Marital Status'] + np.random.normal(0, 0.05, size=len(df))\n",
        "\n",
        "# Beautiful colormap\n",
        "colors = plt.cm.viridis(x_encoded / x_encoded.max())\n",
        "\n",
        "plt.scatter(\n",
        "    x_jitter,\n",
        "    y_jitter,\n",
        "    s=40,\n",
        "    alpha=0.75,\n",
        "    c=colors,\n",
        "    edgecolor=\"black\",\n",
        "    linewidth=0.4\n",
        ")\n",
        "\n",
        "plt.xticks(df[target_col].cat.codes, df[target_col])\n",
        "plt.xlabel(\"Target\")\n",
        "plt.ylabel(\"Marital Status\")\n",
        "plt.title(\"Marital Status by Target\", fontsize=12, fontweight='bold')\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-marital-status",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "@fig-marital-status shows the distribution of marital status across the three target groups using a jittered scatter plot. While marital status achieved statistical significance in the ANOVA test (p < 0.001), its effect size is negligible (η² = 0.009), explaining less than 1% of the variance in student outcomes. This small effect is evident in the plot, where points align in nearly identical horizontal bands for each category, indicating that the marital status profiles are essentially the same among Dropout, Enrolled, and Graduate students.\n",
        "The visual clearly shows that the vast majority of students are single, which is expected given the typical age range of university students. The less common marital statuses married, divorced, widower, facto union, and legally separated—appear only sporadically and are spread evenly across the three groups, further confirming the lack of meaningful differences.\n"
      ],
      "id": "91ac5f3b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mother-qualification\n",
        "#| fig-cap: Mother's qualification by student outcome\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(\n",
        "    x=target_col,\n",
        "    y=\"Mother's qualification\",\n",
        "    data=df,\n",
        "    palette=['#F46968', '#BCDCED', '#31709D']\n",
        ")\n",
        "plt.title(\"Mother's qualification by Target\", fontsize=12, fontweight='bold')\n",
        "plt.grid(alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-mother-qualification",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure 15 shows the Mother’s qualification across the three target groups. Enrolled and  Graduate distributions are identical. The three groups display nearly identical distributions with all median at approximately 19. However, the Dropout group shows a slightly wider interquartile range, extending lower compared to the two other groups. They reach similar limits and there are no outliers.\n",
        "\n",
        "While the three distributions are very similar, the dropout category shows slightly fewer students with mothers who have very  low qualifications, but this is minimal.  The overall similarity indicates that a mother's qualification has little influence on whether students complete their studies. This is also supported by the small effect size.\n",
        "\n",
        "\n",
        "### 6.2.4 - Key Findings for Demographic & Socioeconomic Background \n",
        "\n",
        "Younger students are more likely to graduate while older students face higher dropout risk. The dropout group also shows the widest age variation. This suggests that older students may face competing life responsibilities that interfere with their studies.\n",
        "Female students graduate at slightly higher rates, making up a larger proportion of both enrolled and graduate groups compared to dropouts (50-50 split).\n",
        "\n",
        "\n",
        "Graduates have a noticeably higher proportion of scholarship recipients compared to dropouts and enrolled students, where the majority receive no scholarship. This suggests financial support helps complete their studies. \n",
        "Dropouts have the largest proportion of students who are debtors. Enrolled students include some debtors but fewer than Dropout students. Graduates have almost no debt. Financial pressure is strongly associated with dropout, while financial stability is associated with graduation.\n",
        "All three groups show identical distributions. Other marital statuses appear equally as outliers across all categories. Marital status has no relationship with student outcomes and no predictive value.\n",
        "The three groups display nearly identical distributions with medians around 19. While the dropout group shows a slightly wider spread extending lower, the difference is minimal. Mother’s qualification has little to no influence on whether students complete their studies.\n",
        "\n",
        "\n",
        "## 7 - Predictive Modelling\n",
        "\n",
        "In this section, we begin building a predictive model aimed at understanding which factors are most strongly associated with student dropout. Our goal is not only to classify students into the three outcome categories (Dropout, Enrolled, Graduate), but also to identify which variables contribute most to the risk of dropping out.\n",
        "\n",
        "We train a Random Forest classifier as a first baseline model. This allows us to evaluate predictive performance and obtain a first indication of which features may be important. To further interpret and validate these results, we use LIME explanations, both at the individual level (example students) and globally across multiple samples.\n",
        "\n",
        "This modelling part is therefore an exploratory step toward understanding dropout risk: the aim is to identify meaningful patterns, highlight influential academic or demographic factors, and evaluate which features could be most relevant for predicting student success or failure. Later, these insights can be refined and made more specific to dropout prediction.\n"
      ],
      "id": "1034ed64"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: classification-model\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Create working dataframe\n",
        "df_model = df.copy()\n",
        "\n",
        "# Remove rows with missing values\n",
        "df_model = df_model.dropna()\n",
        "\n",
        "# Separate features and target\n",
        "X = df_model.drop('Target', axis=1)\n",
        "y = df_model['Target']\n",
        "\n",
        "# Encode categorical variables\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "label_encoders = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Split data with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Compute class weights to handle imbalance\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "# Train Random Forest model with class weights\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,  \n",
        "    max_depth=10,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "y_pred_proba = rf_model.predict_proba(X_test)\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "display(Markdown(\"\\n## 7.1 - Model Performance\\n\"))\n",
        "\n",
        "# Accuracy as a formatted statement\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "display(Markdown(f\"**Overall Accuracy:** {accuracy:.3f} ({accuracy*100:.1f}%)\"))\n",
        "\n",
        "# Classification Report as DataFrame\n",
        "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "report_df = pd.DataFrame(report_dict).transpose()\n",
        "\n",
        "# Format the dataframe nicely\n",
        "report_df = report_df.round(3)\n",
        "if 'support' in report_df.columns:\n",
        "    report_df['support'] = report_df['support'].astype(int)\n",
        "\n",
        "# Main classes only\n",
        "main_classes_df = report_df.loc[['Dropout', 'Enrolled', 'Graduate']].copy()\n",
        "display(Markdown(\"\\n### Performance by Class\\n\"))\n",
        "display(main_classes_df)\n",
        "\n",
        "# Summary metrics\n",
        "summary_df = report_df.loc[['accuracy', 'macro avg', 'weighted avg']].copy()\n",
        "display(Markdown(\"\\n### Global Performance\\n\"))\n",
        "display(summary_df)\n",
        "\n",
        "# Class distribution comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "y_test.value_counts().sort_index().plot(kind='bar', ax=axes[0], color='skyblue')\n",
        "axes[0].set_title('True Class Distribution (Test Set)')\n",
        "axes[0].set_xlabel('Class')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "pd.Series(y_pred).value_counts().sort_index().plot(kind='bar', ax=axes[1], color='lightcoral')\n",
        "axes[1].set_title('Predicted Class Distribution')\n",
        "axes[1].set_xlabel('Class')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "The Random Forest classifier achieved an overall accuracy of 67.3%, correctly predicting students' academic performance in approximately two-thirds of cases. However, accuracy by itself is not enough; it is necessary to examine the model's performance for each class.\n",
        "\n",
        "**Performance varies significantly across the three outcomes.** The model performs best at identifying Dropout and Graduate students, with F1-scores of 0.707 and 0.773 respectively. However, it struggles with the Enrolled category, achieving only an F1-score of 0.392. This means that the model has difficulty distinguishing between currently enrolled students and those who will drop out or graduate.\n",
        "\n",
        "**The confusion matrix reveals specific prediction patterns.** The model correctly identifies 185 out of 284 dropouts (65.1% recall) and 338 out of 442 graduates (76.5% recall). The struggle with Enrolled students is evident: only 73 out of 159 (45.9%) are correctly classified, with many being misclassified as either future graduates (53 cases) or potential dropouts (33 cases). This reflects the inherent difficulty in predicting outcomes for students still in progress, their final status remains uncertain until they complete their program or drop out.\n",
        "\n",
        "**Handling class imbalance.** With a smaller number of Enrolled students in the dataset (159 vs 284 Dropouts and 442 Graduates), the model has less training data to learn patterns for this group. To address this imbalance, we used the `class_weight='balanced'` parameter in the Random Forest model, which automatically adjusts weights inversely proportional to the frequency of classes.  This ensures the model gives equal attention to all three classes during training rather than being biased toward the majority class. While this helps mitigate the imbalance, the fundamental challenge remains: enrolled students represent an \"in-between\" state that is harder to characterize than the final outcomes of dropping out or graduating.\n",
        "\n",
        "**Predicted distribution.** The last figure shows the predicted distribution of classes after applying the model. Despite the class imbalance in the original data, the model with balanced class weights manages to capture quite well the real distribution, proving that the balancing strategy is effective.\n",
        "\n",
        "\"\"\"))\n",
        "\n",
        "# ============================================================================\n",
        "# RANDOM FOREST FEATURE IMPORTANCE\n",
        "# ============================================================================\n",
        "\n",
        "display(Markdown(\"\\n## 7.2 - Random Forest Feature Importance\\n\"))\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Random Forest models calculate feature importance by measuring how much each feature contributes to reducing prediction error across all decision trees in the forest. Features that consistently lead to better splits and more accurate predictions receive higher importance scores. It measures each variable’s contribution to prediction while considering all other features, capturing interactions and non-linear relationships.\n",
        "\"\"\"))\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "display(Markdown(\"\\n### 7.2.1-Top 15 Most Important Features (Random Forest)\\n\"))\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_15 = feature_importance.head(15)\n",
        "plt.barh(range(len(top_15)), top_15['importance'])\n",
        "plt.yticks(range(len(top_15)), top_15['feature'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 15 Feature Importance (Random Forest)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Even though Random Forest differs from ANOVA by evaluating variables in combination rather than in isolation, results obtained in both cases show that academic performance variables (_1st and 2nd semester grade_) are dominant.\n",
        "\n",
        "Variables like financial or administrative factors showed weak effects in ANOVA but appear among the top 15 Random Forest  features because they interact with academic variables to improve predictions. Therefore, these variables reveal conditional effects such as financial stress that can increase academic risk when performance is already low. Thus, these interactions enhance both predictive accuracy and the interpretation of underlying patterns in the data.\n",
        "\"\"\"))\n",
        "\n",
        "# Create a custom discretizer that respects class distributions\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train.values,\n",
        "    feature_names=X.columns.tolist(),\n",
        "    class_names=[str(c) for c in sorted(y.unique())],\n",
        "    mode='classification',\n",
        "    random_state=42,\n",
        "    discretize_continuous=True,  # Better for imbalanced data\n",
        "    sample_around_instance=True  # More focused local sampling\n",
        ")\n",
        "\n",
        "# Also create a wrapper that can handle class weights in predictions\n",
        "def balanced_predict_proba(X_sample):\n",
        "    \"\"\"Prediction function that applies class weights to probabilities\"\"\"\n",
        "    probs = rf_model.predict_proba(X_sample)\n",
        "    \n",
        "    # Optional: Apply class weight adjustment to probabilities\n",
        "    # This helps LIME understand the balanced decision boundaries\n",
        "    weights = np.array([class_weight_dict[c] for c in sorted(y.unique())])\n",
        "    adjusted_probs = probs * weights\n",
        "    \n",
        "    # Renormalize\n",
        "    adjusted_probs = adjusted_probs / adjusted_probs.sum(axis=1, keepdims=True)\n",
        "    \n",
        "    return adjusted_probs\n",
        "\n",
        "# ============================================================================\n",
        "# PART 1: INDIVIDUAL EXAMPLES (One per class for illustration)\n",
        "# ============================================================================\n",
        "\n",
        "display(Markdown(\"\\n## 7.3 - LIME Explanations - Individual Examples\\n\"))\n",
        "display(Markdown(\"*Showing one representative example from each class*\\n\"))\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "LIME (Local Interpretable Model-agnostic Explanations) helps us understand why the model made a specific prediction for an individual student. Unlike feature importance which shows what matters globally across all predictions, LIME reveals which features drove the decision for this particular case. The tables below show the top 10 features that most influenced this prediction. Positive weights push the prediction toward the predicted class, while negative weights push against it. The magnitude indicates the strength of influence.\n",
        "\"\"\"))\n",
        "\n",
        "# Select one sample from each class with fixed seed for reproducibility\n",
        "np.random.seed(42)\n",
        "sample_indices = []\n",
        "for class_label in sorted(y.unique()):\n",
        "    class_indices = X_test[y_test == class_label].index\n",
        "    if len(class_indices) > 0:\n",
        "        sample_indices.append(np.random.choice(class_indices, size=1)[0])\n",
        "\n",
        "# Dictionary to store class-specific interpretations\n",
        "# Dictionary to store class-specific interpretations\n",
        "class_interpretations = {\n",
        "    'Dropout': \"\"\"\n",
        "**Interpretation:** This student was correctly identified as at risk of dropping out. The dominant factor is their poor academic performance, with a second semester grade of 10.78 or below strongly pushing toward dropout (large negative weight). Additionally, lacking scholarship support increases dropout risk. Protective factors such as not being in debt, younger age (≤19), and decent first semester grades (≤11) provide some counterbalance, are not enough to overcome poor results in the second semester. This highlights how academic difficulties, especially in later semesters, are key indicators of the risk of dropping out.\n",
        "\"\"\",\n",
        "    'Enrolled': \"\"\"\n",
        "**Interpretation:** This enrolled student presents a highly mixed profile that makes prediction challenging. Poor second semester grades (≤10.78) strongly push toward dropout, while positive factors like having no scholarship (paradoxically protective here), not being in debt, low unemployment rate, and moderate academic performance in the first semester push toward remaining enrolled or graduating. The model shows significant uncertainty, with multiple features pulling in different directions. This reflects the inherent difficulty in predicting outcomes for students still in progress, they're in a transitional state where their trajectory could go either way.\n",
        "\"\"\",\n",
        "    'Graduate': \"\"\"\n",
        "**Interpretation:** This student shows mixed but ultimately positive indicators of academic success. While poor second semester grades (≤10.78) push against graduation, several protective factors dominate: having no scholarship, not being in debt, low unemployment rate, being in a traditional age range (20-25), and having moderate previous qualifications all strongly predict graduation. The combination of financial stability (no debt) and adequate academic preparation outweighs the concerning second semester performance, allowing the model to confidently predict graduation. This demonstrates that graduation is driven by multiple factors beyond just grades alone.\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "for idx, sample_idx in enumerate(sample_indices):\n",
        "    sample = X_test.loc[sample_idx].values\n",
        "    true_label = y_test.loc[sample_idx]\n",
        "    pred_label = rf_model.predict([sample])[0]\n",
        "    pred_proba = rf_model.predict_proba([sample])[0]\n",
        "    \n",
        "    display(Markdown(f\"\\n### Sample {idx + 1}: {true_label}\\n\"))\n",
        "    \n",
        "    sample_info = pd.DataFrame({\n",
        "        'Metric': ['True Label', 'Predicted Label', 'Dropout Prob', 'Enrolled Prob', 'Graduate Prob'],\n",
        "        'Value': [true_label, pred_label, f\"{pred_proba[0]:.3f}\", f\"{pred_proba[1]:.3f}\", f\"{pred_proba[2]:.3f}\"]\n",
        "    })\n",
        "    display(sample_info)\n",
        "    \n",
        "    # Generate LIME explanation with more samples for better stability\n",
        "    exp = explainer.explain_instance(\n",
        "        sample,\n",
        "        rf_model.predict_proba,\n",
        "        num_features=10,\n",
        "        num_samples=5000\n",
        "    )\n",
        "    \n",
        "    # Show explanation as table\n",
        "    explanation_data = []\n",
        "    for feature, weight in exp.as_list():\n",
        "        explanation_data.append({'Feature': feature, 'Weight': f\"{weight:.3f}\"})\n",
        "    \n",
        "    explanation_df = pd.DataFrame(explanation_data)\n",
        "    display(Markdown(\"\\n**Top 10 features influencing this prediction:**\\n\"))\n",
        "    display(explanation_df)\n",
        "    \n",
        "    # Plot explanation with custom title\n",
        "    fig = exp.as_pyplot_figure()\n",
        "    # Update the title to reflect the actual sample\n",
        "    fig.suptitle(f'Local Explanation for Sample {idx + 1}: {true_label}\\n(Predicted: {pred_label})', \n",
        "                 fontsize=14, y=0.98)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Add class-specific interpretation\n",
        "    if true_label in class_interpretations:\n",
        "        display(Markdown(class_interpretations[true_label]))\n",
        "\n",
        "# ============================================================================\n",
        "# PART 2: CLASS-WISE LIME ANALYSIS (Multiple samples per class)\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# DETAILED MISCLASSIFICATION ANALYSIS\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "# COMPARISON ACROSS CLASSES\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# PART 3: GLOBAL LIME IMPORTANCE (Optional - overall patterns)\n",
        "# ============================================================================\n",
        "\n",
        "display(Markdown(\"\\n## 7.4 - Global LIME Feature Importance\\n\"))\n",
        "\n",
        "sample_size = len(X_test)\n",
        "sample_indices_global = np.random.choice(len(X_test), size=sample_size, replace=False)\n",
        "\n",
        "lime_weights_global = {feature: [] for feature in X.columns}\n",
        "\n",
        "for i in sample_indices_global:\n",
        "    exp = explainer.explain_instance(\n",
        "        X_test.iloc[i].values,\n",
        "        rf_model.predict_proba,\n",
        "        num_features=len(X.columns)\n",
        "    )\n",
        "    \n",
        "    for feature, weight in exp.as_list():\n",
        "        feature_name = feature.split('<=')[0].split('>')[0].split('=')[0].strip()\n",
        "        for col in X.columns:\n",
        "            if col in feature_name or feature_name in col:\n",
        "                lime_weights_global[col].append(abs(weight))\n",
        "                break\n",
        "\n",
        "# Compute average\n",
        "lime_importance_global = pd.DataFrame({\n",
        "    'feature': list(lime_weights_global.keys()),\n",
        "    'lime_importance': [np.mean(weights) if weights else 0 for weights in lime_weights_global.values()]\n",
        "}).sort_values('lime_importance', ascending=False)\n",
        "\n",
        "display(Markdown(\"\\n### Top 15 Most Important Features (LIME Global)\\n\"))\n",
        "display(lime_importance_global.head(15).round(4))\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "**Global LIME Feature Importance:** When aggregating LIME explanations across all three classes (Dropout, Enrolled, and Graduate), we obtain a global view of which features most consistently influence the model's predictions regardless of outcome. The ranking remains remarkably similar to the dropout-specific analysis, with second semester grades (0.0583) again dominating as the single most important predictor. Scholarship holder status (0.0527) and debtor status (0.0369) maintain their strong positions, reinforcing that financial factors and academic performance are universally important across all prediction scenarios.\n",
        "\n",
        "The consistency between dropout-specific and global feature importance suggests that the same fundamental factors drive all student outcomes, just in different directions. Strong academic performance and financial stability predict graduation, while their absence predicts dropout. Enrolled students fall somewhere in between, exhibiting mixed patterns of these key indicators. This global perspective validates our earlier findings and demonstrates that interventions targeting academic support and financial aid would benefit students across all outcome categories, not just those at risk of dropping out.\n",
        "\"\"\"))\n",
        "\n",
        "display(Markdown(\"\\n### Where Does the Model Struggle?\\n\"))\n",
        "\n",
        "# Analyze confusion patterns in LIME sample\n",
        "confusion_patterns = {\n",
        "    'Dropout': {'predicted_as': {'Dropout': 0, 'Enrolled': 0, 'Graduate': 0}},\n",
        "    'Enrolled': {'predicted_as': {'Dropout': 0, 'Enrolled': 0, 'Graduate': 0}},\n",
        "    'Graduate': {'predicted_as': {'Dropout': 0, 'Enrolled': 0, 'Graduate': 0}}\n",
        "}\n",
        "\n",
        "# Count predictions for each sample in the global LIME analysis\n",
        "for idx in sample_indices_global:\n",
        "    true_label = y_test.iloc[idx]\n",
        "    sample = X_test.iloc[idx].values\n",
        "    pred_label = rf_model.predict([sample])[0]\n",
        "    confusion_patterns[true_label]['predicted_as'][pred_label] += 1\n",
        "\n",
        "# Create confusion matrix for LIME sample\n",
        "confusion_display_data = []\n",
        "for true_class in sorted(y.unique()):\n",
        "    row = {'True Class': true_class}\n",
        "    for pred_class in sorted(y.unique()):\n",
        "        count = confusion_patterns[true_class]['predicted_as'][pred_class]\n",
        "        total = sum(confusion_patterns[true_class]['predicted_as'].values())\n",
        "        row[f'Pred: {pred_class}'] = f\"{count} ({count/total*100:.0f}%)\"\n",
        "    confusion_display_data.append(row)\n",
        "\n",
        "confusion_display_df = pd.DataFrame(confusion_display_data)\n",
        "display(Markdown(\"\\n**Confusion patterns in LIME sample:**\\n\"))\n",
        "display(confusion_display_df)\n",
        "\n",
        "# Identify problematic patterns\n",
        "display(Markdown(\"\\n**Key Issues:**\\n\"))\n",
        "\n",
        "for true_class in sorted(y.unique()):\n",
        "    total = sum(confusion_patterns[true_class]['predicted_as'].values())\n",
        "    correct = confusion_patterns[true_class]['predicted_as'][true_class]\n",
        "    \n",
        "    if correct / total < 0.7:  # Less than 70% accuracy\n",
        "        main_confusion = max(\n",
        "            [(pred, count) for pred, count in confusion_patterns[true_class]['predicted_as'].items() if pred != true_class],\n",
        "            key=lambda x: x[1]\n",
        "        )\n",
        "        display(Markdown(\n",
        "            f\"- **{true_class}** students are often misclassified as **{main_confusion[0]}** \"\n",
        "            f\"({main_confusion[1]}/{total} cases, {main_confusion[1]/total*100:.0f}%)\"\n",
        "        ))\n",
        "\n",
        "display(Markdown(\"\"\"Due to the class imbalance the model can struggle, since most of the cases people are Graduated, there is a bias towards that group in the data, hence, it is more likelt to predict that for cases when it is hesitant.\"\"\"))"
      ],
      "id": "classification-model",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(Markdown(\"\\n## 7.5 - Feature Importance Comparison: Random Forest, LIME, and ANOVA\\n\"))\n",
        "\n",
        "# Prepare data from all three methods\n",
        "comparison_all = feature_importance.merge(lime_importance_global, on='feature', how='left')\n",
        "comparison_all['lime_importance'] = comparison_all['lime_importance'].fillna(0)\n",
        "\n",
        "# Add ANOVA results (eta_sq) - need to match feature names\n",
        "anova_dict = anova_df['eta_sq'].to_dict()\n",
        "comparison_all['anova_eta_sq'] = comparison_all['feature'].map(anova_dict).fillna(0)\n",
        "\n",
        "# Normalize all importance scores to 0-1 range for fair comparison\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "comparison_all['rf_normalized'] = scaler.fit_transform(comparison_all[['importance']])\n",
        "comparison_all['lime_normalized'] = scaler.fit_transform(comparison_all[['lime_importance']])\n",
        "comparison_all['anova_normalized'] = scaler.fit_transform(comparison_all[['anova_eta_sq']])\n",
        "\n",
        "# Select top 15 features by Random Forest importance\n",
        "top_features = comparison_all.nlargest(15, 'importance')\n",
        "\n",
        "# Create comparison plot\n",
        "plt.figure(figsize=(14, 8))\n",
        "x = np.arange(len(top_features))\n",
        "width = 0.25\n",
        "\n",
        "plt.barh(x - width, top_features['rf_normalized'], width, label='Random Forest', alpha=0.8)\n",
        "plt.barh(x, top_features['lime_normalized'], width, label='LIME', alpha=0.8)\n",
        "plt.barh(x + width, top_features['anova_normalized'], width, label='ANOVA (η²)', alpha=0.8)\n",
        "\n",
        "plt.yticks(x, top_features['feature'])\n",
        "plt.xlabel('Normalized Importance Score (0-1)')\n",
        "plt.title('Feature Importance Comparison: Random Forest vs LIME vs ANOVA')\n",
        "plt.legend()\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "**Comparing Three Approaches to Feature Importance:**\n",
        "\n",
        "This comparison reveals how different methods assess feature importance:\n",
        "\n",
        "- **Random Forest (blue)**: Global feature importance based on how much each feature reduces prediction error across all decision trees. Captures complex interactions and non-linear relationships.\n",
        "\n",
        "- **LIME (orange)**: Local feature importance averaged across 100 samples from each class. Explains individual predictions by measuring how changes in feature values affect model outputs locally.\n",
        "\n",
        "- **ANOVA η² (green)**: Statistical effect size measuring the proportion of variance each feature explains in the target variable. Captures linear relationships and univariate associations.\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "All three methods agree that **2nd semester grades** and **1st semester grades** are the most important predictors, validating academic performance as the dominant factor. However, they diverge on other features:\n",
        "\n",
        "- **Scholarship holder** and **Debtor** rank highly in LIME (practical prediction influence) but lower in Random Forest, suggesting these features work through interactions rather than independently.\n",
        "- **ANOVA** identifies strong linear relationships (high η² for grades) but may underestimate features that work through complex interactions.\n",
        "- **Random Forest** balances both direct effects and feature interactions, providing a comprehensive view of predictive power in the classification context.\n",
        "\n",
        "The convergence on academic performance across all three methods strongly validates its critical role, while divergences highlight how financial and demographic factors contribute through different mechanisms, some through direct effects (captured by ANOVA), others through complex interactions (captured by Random Forest and LIME).\n",
        "\"\"\"))"
      ],
      "id": "f66cf71c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8 - Conclusion \n",
        "This project aimed to understand the factors driving student dropout and academic success. The objective was also to assess whether these outcomes can be predicted accurately, using demographic, socioeconomic, and academic information. Therefore, we were able to answer our research questions through a combination of exploratory data analysis and predictive modeling techniques.\n",
        "\n",
        "1. The first research question therefore focuses on understanding how students’ academic indicators influence their likelihood of graduating or dropping out. a. To what extent do financial factors (debtor status, scholarship holder) affect student retention ?\n",
        "\n",
        "The analysis indicates that academic performance indicators are the strongest determinant of students’ final academic outcomes. Second-semester grades constitute the most decisive factor in distinguishing between students who drop out, remain enrolled, or graduate, as reflected by a very large ANOVA effect size (η² = 0.339). First-semester grades are used as a critical warning signal for identifying students in difficulty. In contrast, indicators of prior academic preparation and course enrollment intensity show lower significance, which suggest that success at the university level relies more heavily on student’s adaptation and performance within the academic environment than on their background before university.\n",
        "Study conditions contribute to outcomes in a more nuanced way. Students attending evening programs face slightly higher dropout risk, likely due to external commitments, while students who relocate for their studies appear more likely to graduate, possibly reflecting stronger educational commitment or fewer competing obligations.\n",
        "Overall, these results point to a critical transition between the first and second semesters. While early academic difficulties can be identified shortly after enrollment, second-semester performance has a major role in determining whether they rebound or lose interest. From a practical perspective, this emphasizes the importance of early academic monitoring and targeted interventions during the first year to reduce dropout risk and promote student success.\n",
        "\n",
        "2. What is the impact of demographic and socioeconomic background on students’ probability of dropping out? a.Which features category, academic, socioeconomic or demographic contribute the most in predicting students’ dropout?\n",
        "\n",
        "While demographic and socioeconomic factors have an impact on student outcomes, it is not as significant compared to academic performance. A clear pattern is observed for _Age at enrollment_: younger students are more likely to graduate, while older students face a higher risk of dropout (η² ≈ 0.065). However, age alone does not determine success. We also observe gender differences, with female students being more likely to remain enrolled and graduate, despite the effect size being moderate. \n",
        "Financial factors also show a clear correlation with student outcomes. Dropping out indeed correlates with indebtedness. Students with debt are more likely to drop out, whereas those who graduate are less concerned about it. These findings support the LIME analysis, which shows that debt and scholarships are two of the most important non-academic factors: financial issues increase the risk of dropping out, while financial support improves retention.\n",
        "Considering other demographic variables such as marital status , application order and parental background, their effect sizes are very small, their distributions are similar across outcome groups and model-based explanations consistently rank them among the least important predictors. Therefore, although they are statistically significant, their effects are negligible and do not meaningfully explain differences in student outcomes.\n",
        "The findings indicate that socioeconomic factors, such as financial stress (_debt_) and financial support (_scholarships_), have a significant impact on student retention. Whereas demographic characteristics such as age and gender play a secondary role compared to academic performance and financial stability.\n",
        "\n",
        "3. Can we accurately predict a student’s final status (Dropout, Enrolled, or Graduate), and which characteristics are most relevant?\n",
        "\n",
        "While the previous research questions focused on identifying and interpreting the individual effects of academic, socioeconomic, and demographic factors on student dropout, the final step in the analysis evaluates these factors jointly within a predictive modeling framework. This shift from explanation to prediction assesses how well a machine-learning model can classify students’ final academic status and which categories of variables contribute most to its performance.\n",
        "\n",
        "The results indicate that predicting students’ final academic status is feasible, though subject to important limitations. The Random Forest model performs well for students with clearly defined outcomes, particularly graduates and dropouts, while currently enrolled students are harder to classify, reflecting the inherent uncertainty of trajectories that are still in progress rather than shortcomings of the model itself.\n",
        "\n",
        "Regarding predictor relevance, a clear hierarchy emerges consistently across all methods. Academic features dominate predictive performance, accounting for approximately 60–70% of explanatory power, with second semester grades and first semester grades ranking highest across ANOVA, Random Forest importance, and LIME explanations. Socioeconomic factors form the second most influential category (around 20–30%), with scholarship holding and debtor status standing out as key non-academic predictors that condition students’ ability to sustain academic performance. Demographic characteristics contribute more modestly, with age at enrollment being the most relevant, while contextual and macroeconomic variables show minimal predictive value. These findings confirm that student outcomes can be predicted with moderate accuracy, primarily driven by academic performance and reinforced by financial stability.\n",
        "\n",
        "---\n",
        "\n",
        "Taken together, this analysis reveals a clear hierarchy in the determinants of student dropout and academic success. This analysis highlights a clear hierarchy in the factors shaping student dropout and academic success. Academic performance stands out as the most influential determinant, followed by financial conditions, while demographic characteristics play a more limited supporting role and contextual variables show little direct impact. Across all methods, semester grades and especially second-semester performance, consistently emerge as the strongest indicators of students’ final outcomes.\n",
        "\n",
        "Rather than being a simple correlation, academic performance appears to be the channel through which other factors take effect. Financial pressure, age-related responsibilities, and study conditions influence students’ capacity to perform academically, which in turn directly affects their likelihood of persisting or dropping out. This makes the first year of study, and particularly the transition to the second semester, a period for intervention\n",
        "\n",
        "The practical implications are clear. Effective dropout prevention should focus on early academic monitoring, combined with targeted financial support, particularly during the first year and before second-semester outcomes become decisive. Encouragingly, the most influential factors identified are also those that institutions can directly address through academic support services, financial aid policies, and early warning systems.\n",
        "\n",
        "Overall, this project demonstrates that student dropout is neither inevitable nor primarily driven by factors beyond institutional control. With timely, data-driven interventions targeting the right factors at the right moments, higher education institutions can meaningfully improve student retention and academic success.\n",
        "\n",
        "\n",
        "## 9 - Appendix\n",
        "Analyzing 100 samples per class to identify robust patterns and handle class imbalance. \n",
        "This following analysis is used to validate the robustness of our results, while the main body focuses on the full dataset to reflect real student outcome distributions."
      ],
      "id": "b79521ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(Markdown(\"\\n## 7.1 - LIME Analysis - Class-Wise Aggregation\\n\"))\n",
        "\n",
        "# Number of samples to analyze per class\n",
        "samples_per_class = 100  # Increased from 20 for more robust analysis\n",
        "\n",
        "# For minority classes, we might want to analyze more\n",
        "class_sample_sizes = {}\n",
        "for class_label in sorted(y.unique()):\n",
        "    class_count = len(X_test[y_test == class_label])\n",
        "    # Use up to 30 samples, or all available if less\n",
        "    class_sample_sizes[class_label] = min(samples_per_class, class_count)\n",
        "\n",
        "# Store LIME weights for each class separately\n",
        "class_lime_weights = {\n",
        "    'Dropout': {feature: [] for feature in X.columns},\n",
        "    'Enrolled': {feature: [] for feature in X.columns},\n",
        "    'Graduate': {feature: [] for feature in X.columns}\n",
        "}\n",
        "\n",
        "# Track prediction accuracy\n",
        "prediction_tracking = {\n",
        "    'Dropout': {'correct': 0, 'incorrect': 0},\n",
        "    'Enrolled': {'correct': 0, 'incorrect': 0},\n",
        "    'Graduate': {'correct': 0, 'incorrect': 0}\n",
        "}\n",
        "\n",
        "# Process each class\n",
        "for class_label in sorted(y.unique()):\n",
        "    \n",
        "    # Get indices for this class\n",
        "    class_indices = X_test[y_test == class_label].index\n",
        "    \n",
        "    # Sample\n",
        "    n_samples = class_sample_sizes[class_label]\n",
        "    sample_indices_class = np.random.choice(class_indices, size=n_samples, replace=False)\n",
        "    \n",
        "    # Generate LIME explanations for each sample\n",
        "    for idx, sample_idx in enumerate(sample_indices_class):\n",
        "        sample = X_test.loc[sample_idx].values\n",
        "        true_label = y_test.loc[sample_idx]\n",
        "        pred_label = rf_model.predict([sample])[0]\n",
        "        \n",
        "        # Track prediction accuracy\n",
        "        if pred_label == true_label:\n",
        "            prediction_tracking[class_label]['correct'] += 1\n",
        "        else:\n",
        "            prediction_tracking[class_label]['incorrect'] += 1\n",
        "        \n",
        "        # Generate LIME explanation with increased samples\n",
        "        exp = explainer.explain_instance(\n",
        "            sample,\n",
        "            rf_model.predict_proba,\n",
        "            num_features=len(X.columns),\n",
        "            num_samples=5000  # More samples = more stable explanations\n",
        "        )\n",
        "        \n",
        "        # Extract weights for this class\n",
        "        for feature, weight in exp.as_list():\n",
        "            feature_name = feature.split('<=')[0].split('>')[0].split('=')[0].strip()\n",
        "            \n",
        "            for col in X.columns:\n",
        "                if col in feature_name or feature_name in col:\n",
        "                    class_lime_weights[class_label][col].append(abs(weight))\n",
        "                    break\n",
        "        \n",
        "\n",
        "class_importance_dfs = {}\n",
        "\n",
        "for class_label in sorted(y.unique()):\n",
        "    importance_list = []\n",
        "    \n",
        "    for feature in X.columns:\n",
        "        weights = class_lime_weights[class_label][feature]\n",
        "        avg_importance = np.mean(weights) if len(weights) > 0 else 0\n",
        "        std_importance = np.std(weights) if len(weights) > 0 else 0\n",
        "        n_appearances = len(weights)\n",
        "        \n",
        "        importance_list.append({\n",
        "            'feature': feature,\n",
        "            'mean_importance': avg_importance,\n",
        "            'std_importance': std_importance,\n",
        "            'appearances': n_appearances\n",
        "        })\n",
        "    \n",
        "    class_df = pd.DataFrame(importance_list).sort_values('mean_importance', ascending=False)\n",
        "    class_importance_dfs[class_label] = class_df\n",
        "    \n",
        "    if class_label == \"Dropout\":\n",
        "        display(Markdown(f\"\\n### Top 15 Features for Predicting: **{class_label}**\\n\"))\n",
        "        display(Markdown(f\"*Based on {n_samples} samples*\\n\"))\n",
        "    \n",
        "    top_15 = class_df.head(15).copy()\n",
        "    top_15['mean_importance'] = top_15['mean_importance'].round(4)\n",
        "    top_15['std_importance'] = top_15['std_importance'].round(4)\n",
        "    if class_label == \"Dropout\":\n",
        "        display(top_15)\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "**Analysis of Dropout Risk Factors:** The aggregated LIME analysis across 100 dropout samples reveals the most influential features driving dropout predictions. Second semester grades emerge as the dominant predictor with a mean importance of 0.0852, significantly higher than any other feature. This is followed by scholarship holder status (0.0525) and debtor status (0.0370), highlighting how financial pressures compound academic struggles. Age at enrollment (0.0283) and first semester grades (0.0234) also play important roles. Notably, all top features appear in all 100 samples, demonstrating their consistent relevance across different dropout cases. The relatively low standard deviations (especially for scholarship holder and debtor status) indicate these features have stable, predictable effects, making them reliable indicators for early intervention systems targeting at-risk students.\n",
        "\"\"\"))\n",
        "\n",
        "display(Markdown(\"\\n## 7.2 - Feature Importance Comparison Across Classes\\n\"))\n",
        "\n",
        "# Get top features from all classes\n",
        "all_top_features = set()\n",
        "for class_label in sorted(y.unique()):\n",
        "    all_top_features.update(class_importance_dfs[class_label].head(10)['feature'].tolist())\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_data = []\n",
        "for feature in all_top_features:\n",
        "    row = {'Feature': feature}\n",
        "    for class_label in sorted(y.unique()):\n",
        "        class_df = class_importance_dfs[class_label]\n",
        "        importance = class_df[class_df['feature'] == feature]['mean_importance'].values\n",
        "        row[class_label] = importance[0] if len(importance) > 0 else 0\n",
        "    comparison_data.append(row)\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Sort by average importance\n",
        "comparison_df['avg'] = comparison_df[sorted(y.unique())].mean(axis=1)\n",
        "comparison_df = comparison_df.sort_values('avg', ascending=False).drop('avg', axis=1)\n",
        "\n",
        "# Grouped bar chart\n",
        "fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "plot_features = comparison_df.head(15)['Feature'].tolist()\n",
        "plot_data = comparison_df[comparison_df['Feature'].isin(plot_features)].set_index('Feature')\n",
        "\n",
        "x = np.arange(len(plot_features))\n",
        "width = 0.25\n",
        "colors = ['#F46968', '#BCDCED', '#31709D']\n",
        "\n",
        "for idx, class_label in enumerate(sorted(y.unique())):\n",
        "    values = [plot_data.loc[feat, class_label] for feat in plot_features]\n",
        "    offset = (idx - 1) * width\n",
        "    ax.barh(x + offset, values, width, label=class_label, alpha=0.8, color=colors[idx])\n",
        "\n",
        "ax.set_yticks(x)\n",
        "ax.set_yticklabels(plot_features)\n",
        "ax.set_xlabel('LIME Importance (Mean Absolute Weight)', fontsize=12)\n",
        "ax.set_title('Feature Importance by Class (LIME Analysis)', fontsize=14, fontweight='bold')\n",
        "ax.legend(title='Target Class', fontsize=10)\n",
        "ax.invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "display(Markdown(\"\"\"Derived from aggregated local LIME,  this figure shows which variables influence the most the decision of the model for each predictive class. For the Dropout class, the features with the strongest influence are _Curricular units 2nd sem (grade)_, _Scholarship holder_, _Debtor_ and _Age_ at enrollment. This indicates that the model frequently relies on these variables when producing predictions classified as Dropout. \n",
        "\n",
        "For the Enrolled class, Curricular units 2nd sem (grade), Scholarship holder, Debtor and Curricular units 1st sem (grade) are found to have the highest contributions, indicating a greater reliance upon academic performance and enrollment-related variables in the model decision process.\n",
        "\n",
        "Finally, for the Graduate class, Scholarship holder, Curricular units 2nd sem (grade), Debtor and Curricular units 1st sem (enrolled) are the most influential features. Overall, the results show that several features are consistently influential across all classes, with Age at enrollment being particularly distinctive for the Dropout class compared to the other classes where it appears with slightly lower influence. These contributions reflect aggregated local decision patterns of the model rather than global feature importance or causal relationships.\n",
        "\n",
        "Conversely, some features show lower aggregated LIME contributions across all predicted classes, suggesting that the model relies less frequently on these variables in its local decisions, although this doesn’t mean they are unimportant in certain individual cases. Across all classes, the least influential features are Course, Educational special needs, Unemployment rate and Curricular units 1st sem (enrolled).\n",
        "\n",
        "Overall, these results indicate that the importance of the academic and financial individual characteristics overcomes the importance of contextual characteristics in the predictions. The prevalence of grades and financial variables, like Scholarship holder or Debtor, indicates that the characteristics of individual students have been taken into consideration. On the other hand, environmental characteristics, like Course or Unemployment rate, have less influence over the predictions.\n",
        "\n",
        "\"\"\"))"
      ],
      "id": "6d5d8a2e",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/patriciagoetz/Documents/DataScienceAssignment/venvDS/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}