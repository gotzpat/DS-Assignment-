---
title: "Predicting Student Dropout and Academic Success"
subtitle: "Exploratory Data Analysis"
authors: 
    - "Patricia Gotz"
    - "Lana Kabanni"
date: today
format:
  html:
    theme: cosmo
    toc: true
    toc-depth: 2
    code-fold: true
    code-tools: true
    df-print: paged
    css: styles.css
  pdf:
       # wrapping the code also in the pdf (otherwise, it overflows)
    toc: false
    echo: false          # HIDE code completely in PDF
    include-in-header:
      text: |
        \usepackage{fvextra}
        \DefineVerbatimEnvironment{Highlighting}{Verbatim}{
          commandchars=\\\{\},
          breaklines, breaknonspaceingroup, breakanywhere
        }
    
execute:
  warning: false
  message: false
---

## I. Introduction

This analysis examines data from a Portuguese higher education institution to identify factors that contribute to student dropout and academic success. The dataset contains information on 4,424 students enrolled across various undergraduate programs.


## I.I - Background and Motivation

Student retention and academic success are crucial issues for higher education institutions. Universities increasingly rely on data-driven insights to identify at-risk students and to design early intervention strategies.
We chose this topic because predicting student dropout not only helps optimize institutional resources but also supports students in achieving their academic goals. Understanding the factors that influence academic success, such as socio-economic background, previous academic performance, or family situation, can improve educational policies and personalized support systems.
This subject is particularly meaningful in data science, because we can combine analytical and predictive methods to better understand and prevent student dropout. 


## I.II - Project Goals

The main objective of this project is to identify the factors that influence students to drop out, stay enrolled, or graduate from higher education. The dataset provides detailed information on each student‚Äôs academic performance, socioeconomic background, and demographic profile, offering a comprehensive view of the variables that shape educational outcomes. By the end of our analysis, we seek to identify the most significant combinations of academic and personal factors that influence student success. 
First, our analysis will focus on academic performance, examining how variables such as admission grades, semester evaluations, and course results relate to final outcomes. For instance, we will analyze whether early academic performance can serve as a reliable predictor of future dropout risk. We will then explore the influence of socioeconomic and personal factors, including parental education, occupation, and financial situation, to understand their impact on academic achievement. Lastly,  the dataset will be used to build and evaluate classification models that predict students‚Äô academic status (Dropout, Enrolled, or Graduate). 
In summary, this study combines exploratory analysis, visualization, and predictive modeling to generate actionable insights that help universities detect at-risk students early and strengthen academic success.

## I.III - Research Questions

- I.  How do academic performance indicators and study conditions influence students‚Äô likelihood of graduation or dropout?
- II. What is the impact of demographic and socioeconomic background on students‚Äô probability of dropping out?
- III. Can we accurately predict a student‚Äôs final status (Dropout, Enrolled, or Graduate) based on their demographic, socioeconomic, and academic characteristics. Which are the most relevant among them?

## Data 

## Data Sourcing
The dataset is publicly available on UCI Machine Learning Repository and was created from multiple databases of higher education institutions in Portugal. It is related to enrolled students in different undergraduate programs and shows how different demographic, socioeconomic and academic factors are related to the dropout. Since the data has already been collected and can be directly downloaded from [UCI MLR - Predict Students' Dropout and Academic Success](https://archive.ics.uci.edu/dataset/697/predict%2Bstudents%2Bdropout%2Band%2Bacademic%2Bsuccess){target="_blank"} - [Accessed on 20th October] , there is no need to collect more data via webscraping or APIs. 

## Data Description
The dataset, containing data from a Portuguese higher education institution, is provided as a CSV file, approximately 520 KB in size, and contains detailed information about students‚Äôdemographic, academic and socio-economic characteristics. It includes 4424 student records and 37 variables (features). After reviewing the dataset variables, we removed two irrelevant ones, resulting in 35 relevant variables selected  for analysis.

The clean data is provided as below ..

## Data Loading

```{python}
#| label: setup

# Import libraries
from ucimlrepo import fetch_ucirepo
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 100

# Load data
dataset = fetch_ucirepo(id=697)
X = np.array(dataset.data.features)
y = np.array(dataset.data.targets)

# Create dataframe
col_names = dataset.variables["name"]
df = pd.DataFrame(np.column_stack((X, y)), columns=col_names)

print(f"Dataset shape: {df.shape}")
```

---

## Variable Selection

We selected 35 relevant variables for analysis:

```{python}
#| label: variable-selection

selected_columns = [ 
    "Marital Status", 
    "Application order", 
    "Course", 
    "Daytime/evening attendance", 
    "Previous qualification", 
    "Previous qualification (grade)",
    "Nacionality", 
    "Mother's qualification", 
    "Father's qualification", 
    "Mother's occupation", 
    "Father's occupation", 
    "Admission grade", 
    "Educational special needs", 
    "Gender", 
    "Scholarship holder", 
    "Age at enrollment", 
    "Displaced", 
    "Debtor", 
    "International", 
    "Curricular units 1st sem (credited)", 
    "Curricular units 1st sem (enrolled)", 
    "Curricular units 1st sem (evaluations)",
    "Curricular units 1st sem (approved)", 
    "Curricular units 1st sem (grade)", 
    "Curricular units 1st sem (without evaluations)", 
    "Curricular units 2nd sem (credited)", 
    "Curricular units 2nd sem (enrolled)", 
    "Curricular units 2nd sem (evaluations)", 
    "Curricular units 2nd sem (approved)", 
    "Curricular units 2nd sem (grade)", 
    "Curricular units 2nd sem (without evaluations)", 
    "Unemployment rate", 
    "Inflation rate", 
    "GDP", 
    "Target", 
]

df = df[selected_columns].copy()
print(f"Selected {len(selected_columns)} variables")
```
Through this step, we didn't encounter any difficult challenges. The dataset was already clean and encoded, so we didn't need to perform variable merging, one-hot encoding or ordinal encoding. We only had to convert categorical variables into readable labels to facilitate our visualization analysis.

---

## Preprocessing (Data Cleaning and Wrangling)

One of the most important steps in our project is data cleaning and wrangling. After running the code to check for missing values and undefined numerical data, we found that the dataset contains no missing values, no mistakes and no data entry mistakes.

The dataset was already encoded, and we removed ‚ÄúApplication mode‚Äù and ‚ÄúTuition fees up to date‚Äù variables because they are not relevant to our research questions. Therefore we dropped two columns from the dataset. Ensuring that the numeric columns are numeric, categorical variables such as ‚ÄúGender‚Äù, ‚ÄúDebtor‚Äù, ‚ÄúDisplaced‚Äù , ‚ÄúDaytime/Evening attendance‚Äù were converted to readable string labels for analysis.


```{python}
#| label: data-cleaning

def clean_dataframe(df, col_missing_thresh=0.30, row_missing_thresh=0.50):
    """Clean dataset with missing value handling."""
  
    # Count number of NaNs
    df = df.copy()
    missing = df.isna().sum()
    missing_data = missing[missing > 0]

    if len(missing_data) > 0:
        print(f"\n‚ö†Ô∏è  Missing values found in {len(missing_data)} columns ({missing_data.sum():,} total)\n")
        display(missing_data.to_frame('Count'))
    else:
        print("\n‚úì No missing values found!")

    print(f"\nShape after cleaning: {df.shape}")
    print(f"Missing values: {df.isna().sum().sum()}")
        
    # Drop columns with excessive missing
    col_frac = df.isna().mean()
    drop_cols = col_frac[col_frac > col_missing_thresh].index.tolist()
    if drop_cols:
        df.drop(columns=drop_cols, inplace=True)
    
    # Drop rows with excessive missing
    row_frac = df.isna().mean(axis=1)
    drop_rows = row_frac[row_frac > row_missing_thresh].index
    if len(drop_rows):
        df = df.drop(index=drop_rows).reset_index(drop=True)
    
    # Coerce numeric types
    df = df.apply(lambda s: pd.to_numeric(s, errors="ignore"))
    
    # Impute missing values
    for col in df.select_dtypes(include=[np.number]).columns:
        if df[col].isna().any():
            df[col] = df[col].fillna(df[col].median())
    
    for col in df.select_dtypes(include=["category","object"]).columns:
        if df[col].isna().any():
            mode = df[col].mode(dropna=True)
            if not mode.empty:
                df[col] = df[col].fillna(mode.iloc[0])
    
    return df

df = clean_dataframe(df)
print(f"Shape after cleaning: {df.shape}")
print(f"Missing values: {df.isna().sum().sum()}")
```
Although we had a well-structured and clean dataset, our main challenge was to determine the reliability of our dataset. We verified if there were any missing values, spotting mistakes, and determined irrelevant variables for our analysis. We pursue our cleaning work with the conversion of the categorical variables. Therefore, the reliable dataset was ready to be analyzed.

---

## Target Variable

```{python}
#| label: target-distribution

# Recode target
target_col = "Target"
df[target_col] = df[target_col].replace({
    0: "Dropout", 
    1: "Enrolled", 
    2: "Graduate"
})
df[target_col] = pd.Categorical(
    df[target_col], 
    categories=["Dropout", "Enrolled", "Graduate"], 
    ordered=True
)

# Visualize
fig, ax = plt.subplots(figsize=(8, 5))
target_counts = df[target_col].value_counts()
colors = ['#e74c3c', '#f39c12', '#2ecc71']
bars = ax.bar(range(len(target_counts)), target_counts.values, 
              color=colors, edgecolor='black', linewidth=1.5)

ax.set_xticks(range(len(target_counts)))
ax.set_xticklabels(target_counts.index)
ax.set_ylabel('Number of Students', fontsize=11)
ax.set_title('Student Outcomes Distribution', fontsize=13, fontweight='bold')
ax.grid(axis='y', alpha=0.3)

# Add labels
for i, v in enumerate(target_counts.values):
    ax.text(i, v + 30, f'{v}\n({v/len(df)*100:.1f}%)', 
            ha='center', fontweight='bold')

plt.tight_layout()
plt.show()
```

---


## Correlation Analysis

```{python}
#| label: correlation-matrix
#| fig-height: 12
#| fig-width: 14

# Calculate correlations
corr = df.corr(numeric_only=True)

# Create heatmap
plt.figure(figsize=(16, 14))
mask = np.triu(np.ones_like(corr, dtype=bool), k=1)

sns.heatmap(
    corr, 
    mask=mask,
    cmap='RdBu_r', 
    center=0,
    vmin=-1, 
    vmax=1,
    annot=True, 
    fmt='.2f',
    square=True,
    linewidths=0.5,
    cbar_kws={"shrink": 0.8, "label": "Correlation"}
)

plt.title('Correlation Matrix of Numeric Variables', 
          fontsize=16, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()
```

Based on this correlation analysis, we identified several highly correlated variable pairs that suggest multicollinearity. We excluded 8 redundant semester variables that were highly correlated with other metrics.

---

## Feature Selection

```{python}
#| label: feature-selection

# Remove highly correlated features
columns_to_remove = [
    "Curricular units 1st sem (credited)", 
    "Curricular units 1st sem (evaluations)", 
    "Curricular units 1st sem (approved)",
    "Curricular units 1st sem (without evaluations)",
    "Curricular units 2nd sem (credited)", 
    "Curricular units 2nd sem (evaluations)", 
    "Curricular units 2nd sem (approved)",
    "Curricular units 2nd sem (without evaluations)"
]

df = df.drop(columns=columns_to_remove)
print(f"Removed {len(columns_to_remove)} highly correlated variables")
print(f"Remaining variables: {df.shape[1]}")
```

---

## Outlier Detection

We implemented a type-aware outlier detection strategy that applies different methods based on the nature of each variable:

**Binary variables** (e.g., Gender, Scholarship holder): Outlier detection was skipped entirely, as these variables only contain two valid values (0/1).

**Nominal categorical variables** (e.g., Course, Nationality): No outlier detection applied, as these represent distinct categories without natural ordering. We only reported the number of unique categories present.

**Ordinal categorical variables** (e.g., qualifications, occupations): We reported the number of levels but did not apply outlier detection, as these represent ordered categories rather than continuous measurements.

**Grade variables** (0-200 scale): We checked for values outside the valid range (0-200). According to the dataset documentation, grades in the Portuguese system can range from 0 to 200.

**Count variables** (e.g., enrolled courses): We used a more lenient threshold of 3√óIQR (Interquartile Range) rather than the standard 1.5√óIQR, as count variables naturally exhibit right-skewed distributions where high values may represent legitimate cases (e.g., students enrolling in many courses).

**Continuous variables** (e.g., Age, GDP, Unemployment rate): We applied the standard Tukey method with 1.5√óIQR threshold to identify potential outliers: values below Q1 - 1.5√óIQR or above Q3 + 1.5√óIQR.

This approach ensures that outlier detection is contextually appropriate for each variable type, reducing false positives while identifying genuine data quality issues.

```{python}
#| label: outlier-detection

def detect_outliers_intelligent(df, var_type_dict):
    """Detect outliers based on variable type using simple statistical rules."""
    results = []
    
    # Binary variables - skip
    print("\nüìä Binary variables (skipping outlier detection):")
    for col in var_type_dict.get('binary', []):
        if col in df.columns:
            unique_vals = sorted(df[col].dropna().unique())
            print(f"  - {col}: values = {unique_vals}")
    
    # Nominal categorical
    print("\nüìä Nominal Categorical (no natural order):")
    for col in var_type_dict.get('nominal', []):
        if col not in df.columns:
            continue
        series = df[col].dropna()
        print(f"  - {col}: {len(series.unique())} categories")
    
    # Ordinal categorical
    print("\nüìä Ordinal Categorical (meaningful order):")
    for col in var_type_dict.get('ordinal', []):
        if col not in df.columns:
            continue
        series = df[col].dropna()
        print(f"  - {col}: {len(series.unique())} levels")
    
    # Grade variables (0-200 scale + Z-score)
    print("\nüìä Grade variables (0-200 range + Z-score > 3):")
    for col in var_type_dict.get('grades', []):
        if col not in df.columns:
            continue
        series = df[col].dropna()
        
        # Check range violations
        invalid = ((series < 0) | (series > 200)).sum()
        
        # Check statistical outliers using Z-score
        mean, std = series.mean(), series.std()
        if std > 0:
            z_scores = np.abs((series - mean) / std)
            statistical_outliers = (z_scores > 3).sum()
        else:
            statistical_outliers = 0
        
        total_outliers = invalid + statistical_outliers
        outlier_pct = 100 * total_outliers / len(series) if len(series) > 0 else 0
        
        print(f"  - {col}: {invalid} out-of-range + {statistical_outliers} extreme (Z>3) = "
              f"{total_outliers} total ({outlier_pct:.1f}%)")
        
        if total_outliers > 0:
            results.append({
                'column': col, 'type': 'grade', 
                'issue': 'out_of_range + extreme',
                'count': total_outliers, 'pct': outlier_pct
            })
    
    # Count variables (Z-score > 3)
    print("\nüìä Count variables (Z-score > 3):")
    for col in var_type_dict.get('counts', []):
        if col not in df.columns:
            continue
        series = df[col].dropna()
        if len(series) == 0:
            continue
        
        mean, std = series.mean(), series.std()
        if std > 0:
            z_scores = np.abs((series - mean) / std)
            outliers = (z_scores > 3).sum()
        else:
            outliers = 0
        
        outlier_pct = 100 * outliers / len(series)
        
        print(f"  - {col}: extreme values: {outliers} ({outlier_pct:.1f}%)")
        
        if outliers > 0:
            results.append({
                'column': col, 'type': 'count', 
                'issue': 'extreme_outlier',
                'count': outliers, 'pct': outlier_pct
            })
    
    # Continuous variables (Z-score > 3)
    print("\nüìä Continuous variables (Z-score > 3):")
    for col in var_type_dict.get('continuous', []):
        if col not in df.columns:
            continue
        series = df[col].dropna()
        if len(series) == 0:
            continue
        
        mean, std = series.mean(), series.std()
        if std > 0:
            z_scores = np.abs((series - mean) / std)
            outliers = (z_scores > 3).sum()
        else:
            outliers = 0
        
        outlier_pct = 100 * outliers / len(series)
        
        print(f"  - {col}: extreme values: {outliers} ({outlier_pct:.1f}%)")
        
        if outliers > 0:
            results.append({
                'column': col, 'type': 'continuous', 
                'issue': 'extreme_outlier',
                'count': outliers, 'pct': outlier_pct
            })
    
    return pd.DataFrame(results)

# Define variable types
var_types = {
    'binary': [
        "Daytime/evening attendance", "Educational special needs", 
        "Gender", "Scholarship holder", "Displaced", "Debtor", "International"
    ],
    'nominal': ["Course", "Nacionality"],
    'ordinal': [
        "Marital Status", "Application mode", "Application order",
        "Previous qualification", "Mother's qualification", 
        "Father's qualification", "Mother's occupation", "Father's occupation"
    ],
    'grades': [
        "Previous qualification (grade)", "Admission grade",
        "Curricular units 1st sem (grade)", "Curricular units 2nd sem (grade)"
    ],
    'counts': [
        "Curricular units 1st sem (enrolled)",
        "Curricular units 2nd sem (enrolled)"
    ],
    'continuous': ["Age at enrollment", "Unemployment rate", "Inflation rate", "GDP"]
}

# Run outlier detection
outlier_results = detect_outliers_intelligent(df, var_types)
```

### Outlier Summary

```{python}
#| label: outlier-summary

if not outlier_results.empty:
    outlier_results = outlier_results.sort_values('pct', ascending=False)
    print("\n Detected Issues:")
    outlier_results
    
    # Visualize problematic variables
    for _, row in outlier_results.iterrows():
        col = row['column']
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # Histogram
        df[col].hist(bins=30, ax=ax1, edgecolor='black')
        ax1.set_title(f"Distribution")
        ax1.set_xlabel(col)
        ax1.set_ylabel("Frequency")
        ax1.grid(alpha=0.3)
        
        # Boxplot
        sns.boxplot(y=df[col], ax=ax2)
        ax2.set_title(f"Boxplot ({row['type']})")
        ax2.grid(alpha=0.3, axis='y')
        
        plt.suptitle(f"{col}: {row['count']} potential outliers ({row['pct']:.1f}%)", 
                     fontsize=12, fontweight='bold')
        plt.tight_layout()
        plt.show()
else:
    print("\n‚úÖ No significant outliers detected!")
```

## Feature Importance Analysis

### Methodology

We used one-way ANOVA (Analysis of Variance) to identify which numeric variables show significant differences across the three target groups (Dropout, Enrolled, Graduate). For each variable, we calculated:

- **p-value**: Statistical significance of differences between groups (Œ± = 0.05)
- **Eta-squared (Œ∑¬≤)**: Effect size measure representing the proportion of variance explained by the target variable (ranges from 0 to 1, where higher values indicate stronger association)

Variables with p-value < 0.05 are considered significantly associated with student outcomes and may be strong predictors in classification models.

```{python}
#| label: anova-analysis

# ANOVA for numeric variables
anova_results = {}
numeric_cols = df.select_dtypes(include=np.number).columns

for col in numeric_cols:
    groups = [df.loc[df[target_col] == cat, col].dropna()
              for cat in df[target_col].cat.categories
              if cat in df[target_col].unique()]
    
    # Need at least 2 non-empty groups
    if sum(len(g) > 0 for g in groups) < 2:
        continue
    
    from scipy.stats import f_oneway
    f_val, p_val = f_oneway(*groups)
    
    # Effect size: eta-squared
    grand_mean = df[col].mean()
    ss_between = sum(len(g) * (g.mean() - grand_mean) ** 2 for g in groups)
    ss_total = ((df[col] - grand_mean) ** 2).sum()
    eta_sq = ss_between / ss_total if ss_total > 0 else np.nan
    
    anova_results[col] = {"p_value": p_val, "eta_sq": eta_sq}

# Create results dataframe
anova_df = (pd.DataFrame(anova_results).T
            .sort_values(["p_value", "eta_sq"], ascending=[True, False]))
anova_df["significant"] = anova_df["p_value"] < 0.05

print(f"Significant variables (p < 0.05): {anova_df['significant'].sum()}")
anova_df.head(15)
```

## Top Predictive Variables

### Acadamic Performance Indicators

Our exploratory analysis shows clear and consistent relationships between academic performance measures and student outcomes (Dropout, Enrolled, Graduate). Several patterns emerge across admission grades, semester performance, and course load.

```{python}
#| label: fig-admission-grade
#| fig-cap: "Admission grade by student outcome"

plt.figure(figsize=(8, 5))
sns.boxplot(
    x=target_col,
    y='Admission grade',
    data=df,
    palette=['#e74c3c', '#f39c12', '#2ecc71']
)
plt.title('Admission grade by Target', fontsize=12, fontweight='bold')
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

Students who graduate generally start with higher admission grades than those who drop out, while enrolled students sit in between. Graduates also show a more consistent range of admission grades, suggesting they arrive better prepared. This means that admission grade is an important early indicator of how ready a student is for university. Students with lower admission grades seem more at risk of struggling early on, which can lead to disengagement and eventually dropping out.

```{python}
#| label: fig-st-sem-grade
#| fig-cap: "First semester grade by student outcome"

plt.figure(figsize=(8, 5))
sns.boxplot(
    x=target_col,
    y='Curricular units 1st sem (grade)',
    data=df,
    palette=['#e74c3c', '#f39c12', '#2ecc71']
)
plt.title('Curricular units 1st sem (grade) by Target', fontsize=12, fontweight='bold')
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

First-semester grades are the strongest indicator of student performance in the whole dataset. Graduates consistently have the highest grades, enrolled students fall in the middle, and dropouts have the lowest. 

```{python}
#| label: fig-boxplot-grades
#| fig-cap: "Boxplot of Grades by Target"
plt.figure(figsize=(8, 5))

sns.boxplot(
    x=target_col,
    y='Curricular units 2nd sem (grade)',
    data=df,
    palette=['#e74c3c', '#f39c12', '#2ecc71']
)

plt.title(f"Curricular units 2nd sem (grade) by {target_col}", fontsize=12, fontweight='bold')
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

Second-semester grades show the same trend, though the differences between groups become slightly smaller once first-semester results are considered. This makes first-semester performance an important early warning sign: it shows how well students adjust to university expectations and workload. Students who struggle early often continue to face difficulties, making these grades especially useful for identifying those at risk of dropping out.

```{python}
#| label: fig-1st-sem-enrolled
#| fig-cap: "First semester enrollment by student outcome"

plt.figure(figsize=(8, 5))
sns.boxplot(
    x=target_col,
    y='Curricular units 1st sem (enrolled)',
    data=df,
    palette=['#e74c3c', '#f39c12', '#2ecc71']
)
plt.title('Curricular units 1st sem (enrolled) by Target', fontsize=12, fontweight='bold')
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

Students who graduate tend to enroll in more first-semester courses compared with those who drop out, while enrolled students fall between the two groups. Dropouts show many low values and irregular patterns, suggesting weaker engagement or early difficulties. Graduates not only take a fuller course load but also display more consistency, which reflects stronger academic commitment. Overall, first-semester enrollment load appears to be a useful indicator of student persistence and early academic momentum.

```{python}

#| label: fig-2nd-sem-enrolled
#| fig-cap: "Second semester enrollment by student outcome"

plt.figure(figsize=(8, 5))
sns.boxplot(
    x=target_col,
    y='Curricular units 2nd sem (enrolled)',
    data=df,
    palette=['#e74c3c', '#f39c12', '#2ecc71']
)
plt.title('Curricular units 2nd sem (enrolled) by Target', fontsize=12, fontweight='bold')
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

In the second semester, the same general pattern seen in the first semester remains: graduates enroll in more curricular units, enrolled students sit in the middle, and dropouts take the fewest. 

However, the differences between groups are less pronounced compared to the first semester. The distributions are more compact, and there are fewer extreme values especially among graduates. Students who continue past the first semester tend to adopt a more regular and stable course load. 

### Key Findings: Academic Performance and Study Conditions

**Academic Performance Impact:**
- Students who graduate have significantly higher admission grades (mean: X) compared to dropouts (mean: Y)
- First semester grades show the strongest association with outcomes (Œ∑¬≤ = X), suggesting early academic performance is a critical indicator
- Approved course units in semester 1 differentiate graduates from dropouts more than enrollment numbers

**Study Conditions Impact:**
- Daytime students show X% higher graduation rates compared to evening students
- Application mode significantly affects outcomes (œá¬≤ = X, p < 0.001), with [specific mode] showing highest graduation rates
- Course type is significantly associated with dropout risk, with [specific courses] showing higher retention


### Demographic & Socioeconomic Background


```{python}
#| label: fig-age
#| fig-cap: "Age at enrollment by student outcome"

plt.figure(figsize=(8, 5))
sns.boxplot(
    x=target_col,
    y='Age at enrollment',
    data=df,
    palette=['#e74c3c', '#f39c12', '#2ecc71']
)
plt.title('Age at enrollment by Target', fontsize=12, fontweight='bold')
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

See @fig-age for details.

```{python}
#| label: fig-scholarship
#| fig-cap: "Scholarship holder status by student outcome"

#plt.figure(figsize=(8, 5))
tab = (pd.crosstab(df[target_col], df['Scholarship holder'])
       .apply(lambda r: r / r.sum(), axis=1))
tab = tab.reindex(columns=sorted(tab.columns.tolist()))

tab.plot(kind="bar", stacked=True, 
        color=['#e74c3c', '#2ecc71'], edgecolor='black')
plt.ylabel("Proportion within target group")
plt.title('Scholarship holder by Target', fontsize=12, fontweight='bold')
plt.legend(title='Scholarship holder', labels=['No', 'Yes'], 
         bbox_to_anchor=(1.02, 1), loc="upper left")
plt.ylim(0, 1)
plt.grid(alpha=0.3, axis='y')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()
```

See @fig-scholarship for details.

```{python}
#| label: fig-debtor
#| fig-cap: "Debtor status by student outcome"

tab = (pd.crosstab(df[target_col], df['Debtor'])
       .apply(lambda r: r / r.sum(), axis=1))
tab = tab.reindex(columns=sorted(tab.columns.tolist()))

tab.plot(kind="bar", stacked=True,
        color=['#e74c3c', '#2ecc71'], edgecolor='black')
plt.ylabel("Proportion within target group")
plt.title('Debtor by Target', fontsize=12, fontweight='bold')
plt.legend(title='Debtor', labels=['No', 'Yes'],
         bbox_to_anchor=(1.02, 1), loc="upper left")
plt.ylim(0, 1)
plt.grid(alpha=0.3, axis='y')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()
```

See @fig-debtor for details.

```{python}
#| label: fig-gender
#| fig-cap: "Gender by student outcome"

tab = (pd.crosstab(df[target_col], df['Gender'])
       .apply(lambda r: r / r.sum(), axis=1))
tab = tab.reindex(columns=sorted(tab.columns.tolist()))

tab.plot(kind="bar", stacked=True,
        color=['#e74c3c', '#2ecc71'], edgecolor='black')
plt.ylabel("Proportion within target group")
plt.title('Gender by Target', fontsize=12, fontweight='bold')
plt.legend(title='Gender', labels=['Female', 'Male'],
         bbox_to_anchor=(1.02, 1), loc="upper left")
plt.ylim(0, 1)
plt.grid(alpha=0.3, axis='y')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()
```

See @fig-gender for details.

```{python}
#| label: fig-displaced
#| fig-cap: "Displaced status by student outcome"

tab = (pd.crosstab(df[target_col], df['Displaced'])
       .apply(lambda r: r / r.sum(), axis=1))
tab = tab.reindex(columns=sorted(tab.columns.tolist()))

tab.plot(kind="bar", stacked=True,
        color=['#e74c3c', '#2ecc71'], edgecolor='black')
plt.ylabel("Proportion within target group")
plt.title('Displaced by Target', fontsize=12, fontweight='bold')
plt.legend(title='Displaced', labels=['No', 'Yes'],
         bbox_to_anchor=(1.02, 1), loc="upper left")
plt.ylim(0, 1)
plt.grid(alpha=0.3, axis='y')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()
```

See @fig-displaced for details.

```{python}
#| label: fig-prev-qual-grade
#| fig-cap: "Previous qualification grade by student outcome"

plt.figure(figsize=(8, 5))
sns.boxplot(
    x=target_col,
    y='Previous qualification (grade)',
    data=df,
    palette=['#e74c3c', '#f39c12', '#2ecc71']
)
plt.title('Previous qualification (grade) by Target', fontsize=12, fontweight='bold')
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

See @fig-prev-qual-grade for details.

```{python}
#| label: fig-marital-status
#| fig-cap: "Marital status by student outcome"

plt.figure(figsize=(8, 5))
sns.boxplot(
    x=target_col,
    y='Marital Status',
    data=df,
    palette=['#e74c3c', '#f39c12', '#2ecc71']
)
plt.title('Marital Status by Target', fontsize=12, fontweight='bold')
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

See @fig-marital-status for details.

```{python}
#| label: fig-application-order
#| fig-cap: "Application order by student outcome"

plt.figure(figsize=(8, 5))
sns.boxplot(
    x=target_col,
    y='Application order',
    data=df,
    palette=['#e74c3c', '#f39c12', '#2ecc71']
)
plt.title('Application order by Target', fontsize=12, fontweight='bold')
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

See @fig-application-order for details.


```{python}
#| label: fig-daytime-evening
#| fig-cap: "Daytime/evening attendance by student outcome"

tab = (pd.crosstab(df[target_col], df['Daytime/evening attendance'])
       .apply(lambda r: r / r.sum(), axis=1))
tab = tab.reindex(columns=sorted(tab.columns.tolist()))

tab.plot(kind="bar", stacked=True,
        color=['#e74c3c', '#2ecc71'], edgecolor='black')
plt.ylabel("Proportion within target group")
plt.title('Daytime/evening attendance by Target', fontsize=12, fontweight='bold')
plt.legend(title='Attendance', labels=['Evening', 'Daytime'],
         bbox_to_anchor=(1.02, 1), loc="upper left")
plt.ylim(0, 1)
plt.grid(alpha=0.3, axis='y')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()
```

See @fig-daytime-evening for details.

```{python}

#| label: fig-mother-qualification
#| fig-cap: "Mother's qualification by student outcome"

plt.figure(figsize=(8, 5))
sns.boxplot(
    x=target_col,
    y="Mother's qualification",
    data=df,
    palette=['#e74c3c', '#f39c12', '#2ecc71']
)
plt.title("Mother's qualification by Target", fontsize=12, fontweight='bold')
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

See @fig-mother-qualification for details.

### Predictive Modelling

```{python}
#| label: classification-model

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import lime
import lime.lime_tabular
import matplotlib.pyplot as plt
import seaborn as sns

# Use the dataframe after feature selection (columns already removed)
# This df already has the 8 redundant columns removed
print(f"Starting with {df.shape[1]} features (after feature selection)")

# Create working dataframe - use all columns that exist
df_model = df.copy()

# Remove rows with missing values
print(f"Dataset shape before cleaning: {df_model.shape}")
df_model = df_model.dropna()
print(f"Dataset shape after removing missing values: {df_model.shape}")

print(f"\nTarget distribution:")
print(df_model['Target'].value_counts())

# Separate features and target
X = df_model.drop('Target', axis=1)
y = df_model['Target']

print(f"\nUsing {X.shape[1]} features for classification")

# Encode categorical variables
categorical_cols = X.select_dtypes(include=['object', 'category']).columns
label_encoders = {}

print(f"\nüîÑ Encoding {len(categorical_cols)} categorical variables...")
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    label_encoders[col] = le

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nTraining set size: {X_train.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")

# Train Random Forest model
print("\nüå≤ Training Random Forest Classifier...")
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train, y_train)

# Make predictions
y_pred = rf_model.predict(X_test)
y_pred_proba = rf_model.predict_proba(X_test)

# Evaluate model
print("\nüìä Model Performance:")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.show()

# Feature Importance from Random Forest
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nüîç Top 15 Most Important Features (Random Forest):")
print(feature_importance.head(15))

# Plot feature importance
plt.figure(figsize=(10, 8))
top_15 = feature_importance.head(15)
plt.barh(range(len(top_15)), top_15['importance'])
plt.yticks(range(len(top_15)), top_15['feature'])
plt.xlabel('Importance')
plt.title('Top 15 Feature Importance (Random Forest)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# LIME Explainer
print("\nüî¨ Setting up LIME explainer...")
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X.columns.tolist(),
    class_names=[str(c) for c in sorted(y.unique())],
    mode='classification',
    random_state=42
)

# Explain a few predictions
print("\nüí° LIME Explanations for Sample Predictions:")

# Select 3 random test samples
np.random.seed(42)
sample_indices = np.random.choice(X_test.index, size=min(3, len(X_test)), replace=False)

for idx, sample_idx in enumerate(sample_indices):
    sample = X_test.loc[sample_idx].values
    true_label = y_test.loc[sample_idx]
    pred_label = rf_model.predict([sample])[0]
    pred_proba = rf_model.predict_proba([sample])[0]
    
    print(f"\n--- Sample {idx + 1} ---")
    print(f"True Label: {true_label}")
    print(f"Predicted Label: {pred_label}")
    print(f"Prediction Probability: {pred_proba}")
    
    # Generate LIME explanation
    exp = explainer.explain_instance(
        sample,
        rf_model.predict_proba,
        num_features=10
    )
    
    # Show explanation
    print("\nTop 10 features influencing this prediction:")
    for feature, weight in exp.as_list():
        print(f"  {feature}: {weight:.3f}")
    
    # Plot explanation
    fig = exp.as_pyplot_figure()
    plt.tight_layout()
    plt.show()

# Global feature importance from LIME (sample-based)
print("\nüìà Computing global LIME feature importance (sampling 100 instances)...")

# Sample instances for global importance
sample_size = min(100, len(X_test))
sample_indices_global = np.random.choice(len(X_test), size=sample_size, replace=False)

lime_weights = {feature: [] for feature in X.columns}

for i in sample_indices_global:
    exp = explainer.explain_instance(
        X_test.iloc[i].values,
        rf_model.predict_proba,
        num_features=len(X.columns)
    )
    
    for feature, weight in exp.as_list():
        # Extract feature name (LIME returns feature with value range)
        feature_name = feature.split('<=')[0].split('>')[0].split('=')[0].strip()
        # Find matching column (partial match)
        for col in X.columns:
            if col in feature_name or feature_name in col:
                lime_weights[col].append(abs(weight))
                break

# Compute average absolute weight for each feature
lime_importance = pd.DataFrame({
    'feature': list(lime_weights.keys()),
    'lime_importance': [np.mean(weights) if weights else 0 for weights in lime_weights.values()]
}).sort_values('lime_importance', ascending=False)

print("\nüîç Top 15 Most Important Features (LIME Global):")
print(lime_importance.head(15))

# Compare Random Forest vs LIME importance
plt.figure(figsize=(12, 8))
comparison = feature_importance.merge(lime_importance, on='feature', how='left')
comparison['lime_importance'] = comparison['lime_importance'].fillna(0)
top_features = comparison.nlargest(15, 'importance')

x = np.arange(len(top_features))
width = 0.35

plt.barh(x - width/2, top_features['importance'], width, label='Random Forest', alpha=0.8)
plt.barh(x + width/2, top_features['lime_importance'], width, label='LIME', alpha=0.8)

plt.yticks(x, top_features['feature'])
plt.xlabel('Importance Score')
plt.title('Feature Importance Comparison: Random Forest vs LIME')
plt.legend()
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

print("\n‚úÖ Classification and LIME analysis complete!")
print(f"\nFinal model uses {len(X.columns)} features to predict student outcomes.")
```